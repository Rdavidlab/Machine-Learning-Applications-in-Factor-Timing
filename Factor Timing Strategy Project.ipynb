{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bb358e2-874f-461b-8984-1b5f1955accd",
   "metadata": {},
   "source": [
    "# Finance Final Project Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa6975b-08d7-4c78-8ea0-74a696fc119d",
   "metadata": {},
   "source": [
    "**Group 8:** *Raymond David, Hyoju Kang, Jinny Kim, Yukta Butala, Kondareddy Thanigundala*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "7075cad4-7919-48d3-85a1-66e8fbdbe863",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import statsmodels.api as sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6783340-d6fd-4ae6-aacf-d286796c0a5b",
   "metadata": {},
   "source": [
    "### Data Cleaning & Importing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a1632b-9bd1-4523-9f81-0d856e9f8c74",
   "metadata": {},
   "source": [
    "#### *Fama French 5 Factors*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "afc3e6db-b0e7-46d9-be6b-7355be1c04c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ff_data_path = 'Data/F-F_Research_Data_5_Factors_2x3.csv'\n",
    "ff_data = pd.read_csv(ff_data_path)\n",
    "ff_data.columns = [\"Date\", \"Mkt-RF\", \"SMB\", \"HML\", \"RMW\", \"CMA\", \"RF\"]\n",
    "ff_data['Date'] = pd.to_datetime(ff_data['Date'], format='%Y%m')\n",
    "ff_data.set_index('Date', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7543e9-60f4-4590-a580-240ac410d804",
   "metadata": {},
   "source": [
    "#### *S&P Data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "a650a194-388e-42cf-93ed-9adea09337ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_data_path = 'Data/S&Pdata.xlsx'\n",
    "sp_data = pd.read_excel(sp_data_path)\n",
    "sp_data.columns = [\"Date\", \"Index\", \"Dividend\", \"Rfree\"]\n",
    "sp_data['Date'] = pd.to_datetime(sp_data['Date'], format='%Y-%m')\n",
    "sp_data.set_index('Date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "14b4c55c-e021-4908-86f0-7b575018aae4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>Dividend</th>\n",
       "      <th>Rfree</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1871-01-01</th>\n",
       "      <td>4.440000</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>0.004814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1871-02-01</th>\n",
       "      <td>4.500000</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>0.004396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1871-03-01</th>\n",
       "      <td>4.610000</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>0.004137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1871-04-01</th>\n",
       "      <td>4.740000</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>0.004508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1871-05-01</th>\n",
       "      <td>4.860000</td>\n",
       "      <td>0.260000</td>\n",
       "      <td>0.003610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-01</th>\n",
       "      <td>2897.498182</td>\n",
       "      <td>56.839092</td>\n",
       "      <td>0.001606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-01</th>\n",
       "      <td>2982.156000</td>\n",
       "      <td>57.220000</td>\n",
       "      <td>0.001561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-01</th>\n",
       "      <td>2977.680000</td>\n",
       "      <td>57.560000</td>\n",
       "      <td>0.001358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-01</th>\n",
       "      <td>3104.904500</td>\n",
       "      <td>57.900000</td>\n",
       "      <td>0.001272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-01</th>\n",
       "      <td>3176.749524</td>\n",
       "      <td>58.240000</td>\n",
       "      <td>0.001267</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1788 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Index   Dividend     Rfree\n",
       "Date                                        \n",
       "1871-01-01     4.440000   0.260000  0.004814\n",
       "1871-02-01     4.500000   0.260000  0.004396\n",
       "1871-03-01     4.610000   0.260000  0.004137\n",
       "1871-04-01     4.740000   0.260000  0.004508\n",
       "1871-05-01     4.860000   0.260000  0.003610\n",
       "...                 ...        ...       ...\n",
       "2019-08-01  2897.498182  56.839092  0.001606\n",
       "2019-09-01  2982.156000  57.220000  0.001561\n",
       "2019-10-01  2977.680000  57.560000  0.001358\n",
       "2019-11-01  3104.904500  57.900000  0.001272\n",
       "2019-12-01  3176.749524  58.240000  0.001267\n",
       "\n",
       "[1788 rows x 3 columns]"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "ed36a340-8065-4979-9ba5-c5491e2ee4d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mkt-RF</th>\n",
       "      <th>SMB</th>\n",
       "      <th>HML</th>\n",
       "      <th>RMW</th>\n",
       "      <th>CMA</th>\n",
       "      <th>RF</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1963-07-01</th>\n",
       "      <td>-0.39</td>\n",
       "      <td>-0.41</td>\n",
       "      <td>-0.97</td>\n",
       "      <td>0.68</td>\n",
       "      <td>-1.18</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1963-08-01</th>\n",
       "      <td>5.07</td>\n",
       "      <td>-0.80</td>\n",
       "      <td>1.80</td>\n",
       "      <td>0.36</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1963-09-01</th>\n",
       "      <td>-1.57</td>\n",
       "      <td>-0.52</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-0.71</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1963-10-01</th>\n",
       "      <td>2.53</td>\n",
       "      <td>-1.39</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>2.80</td>\n",
       "      <td>-2.01</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1963-11-01</th>\n",
       "      <td>-0.85</td>\n",
       "      <td>-0.88</td>\n",
       "      <td>1.75</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>2.24</td>\n",
       "      <td>0.27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-03-01</th>\n",
       "      <td>2.51</td>\n",
       "      <td>-6.94</td>\n",
       "      <td>-8.85</td>\n",
       "      <td>2.24</td>\n",
       "      <td>-2.37</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-04-01</th>\n",
       "      <td>0.61</td>\n",
       "      <td>-2.56</td>\n",
       "      <td>-0.04</td>\n",
       "      <td>2.42</td>\n",
       "      <td>2.86</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-05-01</th>\n",
       "      <td>0.35</td>\n",
       "      <td>-0.38</td>\n",
       "      <td>-7.72</td>\n",
       "      <td>-1.81</td>\n",
       "      <td>-7.22</td>\n",
       "      <td>0.36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-01</th>\n",
       "      <td>6.46</td>\n",
       "      <td>1.34</td>\n",
       "      <td>-0.26</td>\n",
       "      <td>2.18</td>\n",
       "      <td>-1.62</td>\n",
       "      <td>0.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-07-01</th>\n",
       "      <td>3.21</td>\n",
       "      <td>2.86</td>\n",
       "      <td>4.13</td>\n",
       "      <td>-0.56</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>721 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Mkt-RF   SMB   HML   RMW   CMA    RF\n",
       "Date                                            \n",
       "1963-07-01   -0.39 -0.41 -0.97  0.68 -1.18  0.27\n",
       "1963-08-01    5.07 -0.80  1.80  0.36 -0.35  0.25\n",
       "1963-09-01   -1.57 -0.52  0.13 -0.71  0.29  0.27\n",
       "1963-10-01    2.53 -1.39 -0.10  2.80 -2.01  0.29\n",
       "1963-11-01   -0.85 -0.88  1.75 -0.51  2.24  0.27\n",
       "...            ...   ...   ...   ...   ...   ...\n",
       "2023-03-01    2.51 -6.94 -8.85  2.24 -2.37  0.36\n",
       "2023-04-01    0.61 -2.56 -0.04  2.42  2.86  0.35\n",
       "2023-05-01    0.35 -0.38 -7.72 -1.81 -7.22  0.36\n",
       "2023-06-01    6.46  1.34 -0.26  2.18 -1.62  0.40\n",
       "2023-07-01    3.21  2.86  4.13 -0.56  0.62  0.45\n",
       "\n",
       "[721 rows x 6 columns]"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ff_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6aa59a0-4277-4cd8-be67-f3329db24034",
   "metadata": {},
   "source": [
    "#### *Combining Initial Data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "9c76aaa2-d6cc-499a-a876-530d1e565079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mkt-RF</th>\n",
       "      <th>HML</th>\n",
       "      <th>SMB</th>\n",
       "      <th>RMW</th>\n",
       "      <th>CMA</th>\n",
       "      <th>RF</th>\n",
       "      <th>Index</th>\n",
       "      <th>Dividend</th>\n",
       "      <th>Rfree</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1963-07-01</th>\n",
       "      <td>-0.39</td>\n",
       "      <td>-0.97</td>\n",
       "      <td>-0.41</td>\n",
       "      <td>0.68</td>\n",
       "      <td>-1.18</td>\n",
       "      <td>0.27</td>\n",
       "      <td>69.07</td>\n",
       "      <td>2.20333</td>\n",
       "      <td>0.002733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1963-08-01</th>\n",
       "      <td>5.07</td>\n",
       "      <td>1.80</td>\n",
       "      <td>-0.80</td>\n",
       "      <td>0.36</td>\n",
       "      <td>-0.35</td>\n",
       "      <td>0.25</td>\n",
       "      <td>70.98</td>\n",
       "      <td>2.20667</td>\n",
       "      <td>0.002832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1963-09-01</th>\n",
       "      <td>-1.57</td>\n",
       "      <td>0.13</td>\n",
       "      <td>-0.52</td>\n",
       "      <td>-0.71</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.27</td>\n",
       "      <td>72.85</td>\n",
       "      <td>2.21000</td>\n",
       "      <td>0.002849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1963-10-01</th>\n",
       "      <td>2.53</td>\n",
       "      <td>-0.10</td>\n",
       "      <td>-1.39</td>\n",
       "      <td>2.80</td>\n",
       "      <td>-2.01</td>\n",
       "      <td>0.29</td>\n",
       "      <td>73.03</td>\n",
       "      <td>2.23333</td>\n",
       "      <td>0.002906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1963-11-01</th>\n",
       "      <td>-0.85</td>\n",
       "      <td>1.75</td>\n",
       "      <td>-0.88</td>\n",
       "      <td>-0.51</td>\n",
       "      <td>2.24</td>\n",
       "      <td>0.27</td>\n",
       "      <td>72.62</td>\n",
       "      <td>2.25667</td>\n",
       "      <td>0.002923</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Mkt-RF   HML   SMB   RMW   CMA    RF  Index  Dividend     Rfree\n",
       "Date                                                                       \n",
       "1963-07-01   -0.39 -0.97 -0.41  0.68 -1.18  0.27  69.07   2.20333  0.002733\n",
       "1963-08-01    5.07  1.80 -0.80  0.36 -0.35  0.25  70.98   2.20667  0.002832\n",
       "1963-09-01   -1.57  0.13 -0.52 -0.71  0.29  0.27  72.85   2.21000  0.002849\n",
       "1963-10-01    2.53 -0.10 -1.39  2.80 -2.01  0.29  73.03   2.23333  0.002906\n",
       "1963-11-01   -0.85  1.75 -0.88 -0.51  2.24  0.27  72.62   2.25667  0.002923"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_factors_ff = ff_data[[\"Mkt-RF\", \"HML\",\"SMB\",\"RMW\",\"CMA\",\"RF\"]]\n",
    "selected_factors_sp = sp_data[[\"Index\",\"Dividend\",\"Rfree\"]]\n",
    "\n",
    "combined_factors = selected_factors_ff.join(selected_factors_sp, how='inner')\n",
    "combined_factors.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b708cd2c-95a6-4316-b7c1-b3d55e66896b",
   "metadata": {},
   "source": [
    "#### *Load Additional Predictors from Yahoo Finance, FRED Website, and PredictorData2019.csv*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b67b447-cc40-45d3-a412-2e73d8886c2f",
   "metadata": {},
   "source": [
    "##### *Load 10-year Breakeven Inflation, 10-year REAL Interest Rate Data, VIX Data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "a922cde8-c253-4f89-8ef9-844a5731dc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the 10-year Breakeven Inflation\n",
    "t10yie_data = pd.read_csv('T10YIE.csv')\n",
    "\n",
    "# Load the 10-year REAL Interest Rate Data\n",
    "reaintratrearat10y_data = pd.read_csv('Data/REAINTRATREARAT10Y.csv')\n",
    "\n",
    "# Load the Unemployment Rate Data\n",
    "unrate = pd.read_csv('Data/UNRATE.csv')\n",
    "\n",
    "# Load VIX Data\n",
    "vix = pd.read_csv('Data/^VIX.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "b7d99081-aef6-4ed0-8292-68068dd3f09a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            DATE T10YIE\n",
      "0     2003-01-02   1.64\n",
      "1     2003-01-03   1.62\n",
      "2     2003-01-06   1.63\n",
      "3     2003-01-07   1.62\n",
      "4     2003-01-08   1.71\n",
      "...          ...    ...\n",
      "5552  2024-04-15   2.43\n",
      "5553  2024-04-16   2.43\n",
      "5554  2024-04-17   2.39\n",
      "5555  2024-04-18    2.4\n",
      "5556  2024-04-19   2.41\n",
      "\n",
      "[5557 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(t10yie_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "e29b0040-3ae6-430f-b5c0-31d4392a6217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           DATE  REAINTRATREARAT10Y\n",
      "0    1982-01-01            7.623742\n",
      "1    1982-02-01            7.656648\n",
      "2    1982-03-01            7.128993\n",
      "3    1982-04-01            7.408347\n",
      "4    1982-05-01            7.320041\n",
      "..          ...                 ...\n",
      "503  2023-12-01            1.680899\n",
      "504  2024-01-01            1.680871\n",
      "505  2024-02-01            1.616823\n",
      "506  2024-03-01            1.925969\n",
      "507  2024-04-01            1.934547\n",
      "\n",
      "[508 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(reaintratrearat10y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "e7e0e2aa-897a-45d9-a7b8-5621a7f0e4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           DATE  UNRATE\n",
      "0    1948-01-01     3.4\n",
      "1    1948-02-01     3.8\n",
      "2    1948-03-01     4.0\n",
      "3    1948-04-01     3.9\n",
      "4    1948-05-01     3.5\n",
      "..          ...     ...\n",
      "910  2023-11-01     3.7\n",
      "911  2023-12-01     3.7\n",
      "912  2024-01-01     3.7\n",
      "913  2024-02-01     3.9\n",
      "914  2024-03-01     3.8\n",
      "\n",
      "[915 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(unrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "86290d9e-503b-40d2-9c14-e9e8af3344fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Date       Open       High        Low      Close  Adj Close  Volume\n",
      "0    1990-02-01  24.870001  24.870001  19.709999  21.990000  21.990000     0.0\n",
      "1    1990-03-01  21.900000  22.740000  17.620001  19.730000  19.730000     0.0\n",
      "2    1990-04-01  22.840000  24.160000  17.730000  19.520000  19.520000     0.0\n",
      "3    1990-05-01  18.430000  20.139999  16.719999  17.370001  17.370001     0.0\n",
      "4    1990-06-01  17.090000  19.360001  14.720000  15.500000  15.500000     0.0\n",
      "..          ...        ...        ...        ...        ...        ...     ...\n",
      "407  2024-01-01        NaN        NaN        NaN        NaN        NaN     NaN\n",
      "408  2024-02-01        NaN        NaN        NaN        NaN        NaN     NaN\n",
      "409  2024-03-01        NaN        NaN        NaN        NaN        NaN     NaN\n",
      "410  2024-04-01        NaN        NaN        NaN        NaN        NaN     NaN\n",
      "411  2024-04-19  21.330000  21.360001  18.170000  18.709999  18.709999     0.0\n",
      "\n",
      "[412 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "print(vix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "2dba1013-6d4c-4ac3-aa08-8d69f0937e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date        VIX\n",
      "0  1990-02-01  21.990000\n",
      "1  1990-03-01  19.730000\n",
      "2  1990-04-01  19.520000\n",
      "3  1990-05-01  17.370001\n",
      "4  1990-06-01  15.500000\n"
     ]
    }
   ],
   "source": [
    "vix = vix.dropna(subset=['Adj Close'])\n",
    "\n",
    "# Keep only the 'Date' and 'Adj Close' columns\n",
    "vix = vix[['Date', 'Adj Close']]\n",
    "\n",
    "# Rename the 'Adj Close' column to 'VIX'\n",
    "vix.rename(columns={'Adj Close': 'VIX'}, inplace=True)\n",
    "\n",
    "print(vix.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feffff23-b4dc-4f96-aadd-807feea167fe",
   "metadata": {},
   "source": [
    "##### *Load CCI Data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "19ed4858-dccd-4570-9223-faff998b4a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Date    CCI\n",
      "0  1960-01  101.6\n",
      "1  1960-02  101.4\n",
      "2  1960-03  101.1\n",
      "3  1960-04  101.0\n",
      "4  1960-05  101.1\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the Excel file\n",
    "file_path = 'Data/CCI.xlsx'  # Update this to the path where your file is stored\n",
    "CCI = pd.read_excel(file_path)\n",
    "\n",
    "# Display the first few rows of the DataFrame to confirm it's loaded correctly\n",
    "print(CCI.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4c8747-4621-43f2-8c00-5da14f9b6cba",
   "metadata": {},
   "source": [
    "##### *Load PredictorData2019.csv*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "bc64949c-f0b3-48e4-b73c-fc7e2e7c52a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DATE</th>\n",
       "      <th>Index</th>\n",
       "      <th>D12</th>\n",
       "      <th>E12</th>\n",
       "      <th>b/m</th>\n",
       "      <th>tbl</th>\n",
       "      <th>AAA</th>\n",
       "      <th>BAA</th>\n",
       "      <th>lty</th>\n",
       "      <th>ntis</th>\n",
       "      <th>Rfree</th>\n",
       "      <th>infl</th>\n",
       "      <th>ltr</th>\n",
       "      <th>corpr</th>\n",
       "      <th>svar</th>\n",
       "      <th>csp</th>\n",
       "      <th>CRSP_SPvw</th>\n",
       "      <th>CRSP_SPvwx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1/31/1900</td>\n",
       "      <td>6.10</td>\n",
       "      <td>0.2175</td>\n",
       "      <td>0.48</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003278</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002432</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2/28/1900</td>\n",
       "      <td>6.21</td>\n",
       "      <td>0.2250</td>\n",
       "      <td>0.48</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002886</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000838</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3/31/1900</td>\n",
       "      <td>6.26</td>\n",
       "      <td>0.2325</td>\n",
       "      <td>0.48</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003241</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001222</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4/30/1900</td>\n",
       "      <td>6.34</td>\n",
       "      <td>0.2400</td>\n",
       "      <td>0.48</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002776</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001425</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5/31/1900</td>\n",
       "      <td>6.04</td>\n",
       "      <td>0.2475</td>\n",
       "      <td>0.48</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002370</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001691</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        DATE  Index     D12   E12  b/m  tbl  AAA  BAA  lty  ntis     Rfree  \\\n",
       "0  1/31/1900   6.10  0.2175  0.48  NaN  NaN  NaN  NaN  NaN   NaN  0.003278   \n",
       "1  2/28/1900   6.21  0.2250  0.48  NaN  NaN  NaN  NaN  NaN   NaN  0.002886   \n",
       "2  3/31/1900   6.26  0.2325  0.48  NaN  NaN  NaN  NaN  NaN   NaN  0.003241   \n",
       "3  4/30/1900   6.34  0.2400  0.48  NaN  NaN  NaN  NaN  NaN   NaN  0.002776   \n",
       "4  5/31/1900   6.04  0.2475  0.48  NaN  NaN  NaN  NaN  NaN   NaN  0.002370   \n",
       "\n",
       "   infl  ltr  corpr      svar  csp  CRSP_SPvw  CRSP_SPvwx  \n",
       "0   NaN  NaN    NaN  0.002432  NaN        NaN         NaN  \n",
       "1   NaN  NaN    NaN  0.000838  NaN        NaN         NaN  \n",
       "2   NaN  NaN    NaN  0.001222  NaN        NaN         NaN  \n",
       "3   NaN  NaN    NaN  0.001425  NaN        NaN         NaN  \n",
       "4   NaN  NaN    NaN  0.001691  NaN        NaN         NaN  "
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictors = pd.read_csv('Data/PredictorData2019.csv')\n",
    "predictors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "241c7667-6ca0-4785-9678-8f7b45ddc0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'DATE' column to datetime\n",
    "predictors['DATE'] = pd.to_datetime(predictors['DATE'], errors='coerce')  # 'coerce' to handle any non-convertible values\n",
    "\n",
    "# Drop rows where 'DATE' could not be converted (if there were any)\n",
    "predictors = predictors.dropna(subset=['DATE'])\n",
    "\n",
    "# Adjust the 'DATE' to the first of the month\n",
    "predictors['DATE'] = predictors['DATE'].apply(lambda d: d - pd.offsets.MonthEnd(1) + pd.offsets.Day(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "0cbd33c1-867e-4844-bef4-f76e49fb88e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>D12</th>\n",
       "      <th>E12</th>\n",
       "      <th>b/m</th>\n",
       "      <th>tbl</th>\n",
       "      <th>AAA</th>\n",
       "      <th>BAA</th>\n",
       "      <th>lty</th>\n",
       "      <th>ntis</th>\n",
       "      <th>Rfree</th>\n",
       "      <th>infl</th>\n",
       "      <th>ltr</th>\n",
       "      <th>corpr</th>\n",
       "      <th>svar</th>\n",
       "      <th>csp</th>\n",
       "      <th>CRSP_SPvw</th>\n",
       "      <th>CRSP_SPvwx</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DATE</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1900-01-01</th>\n",
       "      <td>6.100000</td>\n",
       "      <td>0.217500</td>\n",
       "      <td>0.48</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003278</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002432</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1900-02-01</th>\n",
       "      <td>6.210000</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>0.48</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002886</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000838</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1900-03-01</th>\n",
       "      <td>6.260000</td>\n",
       "      <td>0.232500</td>\n",
       "      <td>0.48</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.003241</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001222</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1900-04-01</th>\n",
       "      <td>6.340000</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.48</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002776</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001425</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1900-05-01</th>\n",
       "      <td>6.040000</td>\n",
       "      <td>0.247500</td>\n",
       "      <td>0.48</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002370</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.001691</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-01</th>\n",
       "      <td>2926.459961</td>\n",
       "      <td>56.838763</td>\n",
       "      <td>133.69</td>\n",
       "      <td>0.237917</td>\n",
       "      <td>0.0195</td>\n",
       "      <td>0.0298</td>\n",
       "      <td>0.0387</td>\n",
       "      <td>0.0163</td>\n",
       "      <td>-0.010244</td>\n",
       "      <td>0.001625</td>\n",
       "      <td>-0.000051</td>\n",
       "      <td>0.0797</td>\n",
       "      <td>0.0738</td>\n",
       "      <td>0.004318</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.016085</td>\n",
       "      <td>-0.018377</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-01</th>\n",
       "      <td>2976.739990</td>\n",
       "      <td>57.219507</td>\n",
       "      <td>132.90</td>\n",
       "      <td>0.233377</td>\n",
       "      <td>0.0189</td>\n",
       "      <td>0.0303</td>\n",
       "      <td>0.0391</td>\n",
       "      <td>0.0170</td>\n",
       "      <td>-0.010959</td>\n",
       "      <td>0.001575</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>-0.0192</td>\n",
       "      <td>-0.0190</td>\n",
       "      <td>0.000605</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.018791</td>\n",
       "      <td>0.017272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-01</th>\n",
       "      <td>3037.560059</td>\n",
       "      <td>57.559879</td>\n",
       "      <td>135.09</td>\n",
       "      <td>0.232261</td>\n",
       "      <td>0.0165</td>\n",
       "      <td>0.0301</td>\n",
       "      <td>0.0392</td>\n",
       "      <td>0.0171</td>\n",
       "      <td>-0.013267</td>\n",
       "      <td>0.001375</td>\n",
       "      <td>0.002286</td>\n",
       "      <td>-0.0052</td>\n",
       "      <td>0.0006</td>\n",
       "      <td>0.001510</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.021621</td>\n",
       "      <td>0.020441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-01</th>\n",
       "      <td>3140.979980</td>\n",
       "      <td>57.900251</td>\n",
       "      <td>137.28</td>\n",
       "      <td>0.223938</td>\n",
       "      <td>0.0154</td>\n",
       "      <td>0.0306</td>\n",
       "      <td>0.0394</td>\n",
       "      <td>0.0181</td>\n",
       "      <td>-0.007907</td>\n",
       "      <td>0.001283</td>\n",
       "      <td>-0.000536</td>\n",
       "      <td>-0.0059</td>\n",
       "      <td>0.0014</td>\n",
       "      <td>0.000306</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.036206</td>\n",
       "      <td>0.033979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-01</th>\n",
       "      <td>3230.780029</td>\n",
       "      <td>58.240623</td>\n",
       "      <td>139.47</td>\n",
       "      <td>0.220116</td>\n",
       "      <td>0.0154</td>\n",
       "      <td>0.0301</td>\n",
       "      <td>0.0388</td>\n",
       "      <td>0.0186</td>\n",
       "      <td>-0.007306</td>\n",
       "      <td>0.001283</td>\n",
       "      <td>-0.000910</td>\n",
       "      <td>-0.0253</td>\n",
       "      <td>-0.0089</td>\n",
       "      <td>0.000502</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.029788</td>\n",
       "      <td>0.028136</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1440 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Index        D12     E12       b/m     tbl     AAA     BAA  \\\n",
       "DATE                                                                           \n",
       "1900-01-01     6.100000   0.217500    0.48       NaN     NaN     NaN     NaN   \n",
       "1900-02-01     6.210000   0.225000    0.48       NaN     NaN     NaN     NaN   \n",
       "1900-03-01     6.260000   0.232500    0.48       NaN     NaN     NaN     NaN   \n",
       "1900-04-01     6.340000   0.240000    0.48       NaN     NaN     NaN     NaN   \n",
       "1900-05-01     6.040000   0.247500    0.48       NaN     NaN     NaN     NaN   \n",
       "...                 ...        ...     ...       ...     ...     ...     ...   \n",
       "2019-08-01  2926.459961  56.838763  133.69  0.237917  0.0195  0.0298  0.0387   \n",
       "2019-09-01  2976.739990  57.219507  132.90  0.233377  0.0189  0.0303  0.0391   \n",
       "2019-10-01  3037.560059  57.559879  135.09  0.232261  0.0165  0.0301  0.0392   \n",
       "2019-11-01  3140.979980  57.900251  137.28  0.223938  0.0154  0.0306  0.0394   \n",
       "2019-12-01  3230.780029  58.240623  139.47  0.220116  0.0154  0.0301  0.0388   \n",
       "\n",
       "               lty      ntis     Rfree      infl     ltr   corpr      svar  \\\n",
       "DATE                                                                         \n",
       "1900-01-01     NaN       NaN  0.003278       NaN     NaN     NaN  0.002432   \n",
       "1900-02-01     NaN       NaN  0.002886       NaN     NaN     NaN  0.000838   \n",
       "1900-03-01     NaN       NaN  0.003241       NaN     NaN     NaN  0.001222   \n",
       "1900-04-01     NaN       NaN  0.002776       NaN     NaN     NaN  0.001425   \n",
       "1900-05-01     NaN       NaN  0.002370       NaN     NaN     NaN  0.001691   \n",
       "...            ...       ...       ...       ...     ...     ...       ...   \n",
       "2019-08-01  0.0163 -0.010244  0.001625 -0.000051  0.0797  0.0738  0.004318   \n",
       "2019-09-01  0.0170 -0.010959  0.001575  0.000783 -0.0192 -0.0190  0.000605   \n",
       "2019-10-01  0.0171 -0.013267  0.001375  0.002286 -0.0052  0.0006  0.001510   \n",
       "2019-11-01  0.0181 -0.007907  0.001283 -0.000536 -0.0059  0.0014  0.000306   \n",
       "2019-12-01  0.0186 -0.007306  0.001283 -0.000910 -0.0253 -0.0089  0.000502   \n",
       "\n",
       "            csp  CRSP_SPvw  CRSP_SPvwx  \n",
       "DATE                                    \n",
       "1900-01-01  NaN        NaN         NaN  \n",
       "1900-02-01  NaN        NaN         NaN  \n",
       "1900-03-01  NaN        NaN         NaN  \n",
       "1900-04-01  NaN        NaN         NaN  \n",
       "1900-05-01  NaN        NaN         NaN  \n",
       "...         ...        ...         ...  \n",
       "2019-08-01  NaN  -0.016085   -0.018377  \n",
       "2019-09-01  NaN   0.018791    0.017272  \n",
       "2019-10-01  NaN   0.021621    0.020441  \n",
       "2019-11-01  NaN   0.036206    0.033979  \n",
       "2019-12-01  NaN   0.029788    0.028136  \n",
       "\n",
       "[1440 rows x 17 columns]"
      ]
     },
     "execution_count": 270,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictors.set_index('DATE', inplace=True)\n",
    "predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "355589d5-cd43-46c8-ac02-628250c94a76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index           0\n",
      "D12             0\n",
      "E12             0\n",
      "b/m           254\n",
      "tbl           240\n",
      "AAA           228\n",
      "BAA           228\n",
      "lty           228\n",
      "ntis          323\n",
      "Rfree           0\n",
      "infl          157\n",
      "ltr           312\n",
      "corpr         312\n",
      "svar            0\n",
      "csp           652\n",
      "CRSP_SPvw     312\n",
      "CRSP_SPvwx    312\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "nan_counts = predictors.isna().sum()\n",
    "\n",
    "# Print the counts of NaNs in each column\n",
    "print(nan_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aad784b0-f608-4d90-b905-b47982fcd20a",
   "metadata": {},
   "source": [
    "#### Remove Predictor columns with more than 1 NUll values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "11559d45-cf91-4662-93fd-fd4f3f14c602",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictors.dropna(axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "cbf412bf-441d-41cc-9194-1e50e4a3418b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>D12</th>\n",
       "      <th>E12</th>\n",
       "      <th>svar</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DATE</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1900-01-01</th>\n",
       "      <td>6.100000</td>\n",
       "      <td>0.217500</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.002432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1900-02-01</th>\n",
       "      <td>6.210000</td>\n",
       "      <td>0.225000</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.000838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1900-03-01</th>\n",
       "      <td>6.260000</td>\n",
       "      <td>0.232500</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.001222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1900-04-01</th>\n",
       "      <td>6.340000</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.001425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1900-05-01</th>\n",
       "      <td>6.040000</td>\n",
       "      <td>0.247500</td>\n",
       "      <td>0.48</td>\n",
       "      <td>0.001691</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-01</th>\n",
       "      <td>2926.459961</td>\n",
       "      <td>56.838763</td>\n",
       "      <td>133.69</td>\n",
       "      <td>0.004318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-01</th>\n",
       "      <td>2976.739990</td>\n",
       "      <td>57.219507</td>\n",
       "      <td>132.90</td>\n",
       "      <td>0.000605</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-01</th>\n",
       "      <td>3037.560059</td>\n",
       "      <td>57.559879</td>\n",
       "      <td>135.09</td>\n",
       "      <td>0.001510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-01</th>\n",
       "      <td>3140.979980</td>\n",
       "      <td>57.900251</td>\n",
       "      <td>137.28</td>\n",
       "      <td>0.000306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-01</th>\n",
       "      <td>3230.780029</td>\n",
       "      <td>58.240623</td>\n",
       "      <td>139.47</td>\n",
       "      <td>0.000502</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1440 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Index        D12     E12      svar\n",
       "DATE                                                \n",
       "1900-01-01     6.100000   0.217500    0.48  0.002432\n",
       "1900-02-01     6.210000   0.225000    0.48  0.000838\n",
       "1900-03-01     6.260000   0.232500    0.48  0.001222\n",
       "1900-04-01     6.340000   0.240000    0.48  0.001425\n",
       "1900-05-01     6.040000   0.247500    0.48  0.001691\n",
       "...                 ...        ...     ...       ...\n",
       "2019-08-01  2926.459961  56.838763  133.69  0.004318\n",
       "2019-09-01  2976.739990  57.219507  132.90  0.000605\n",
       "2019-10-01  3037.560059  57.559879  135.09  0.001510\n",
       "2019-11-01  3140.979980  57.900251  137.28  0.000306\n",
       "2019-12-01  3230.780029  58.240623  139.47  0.000502\n",
       "\n",
       "[1440 rows x 4 columns]"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select Needed Variables\n",
    "predictors = predictors[['Index', 'D12','E12','svar']]\n",
    "predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e730cc-e875-446c-9b39-2a3b4fdd5353",
   "metadata": {},
   "source": [
    "#### Further Clean the Data, Convert the Date columns, and Inner Join Them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "6236ea5e-ed5d-4ce4-a559-ba148a93314a",
   "metadata": {},
   "outputs": [],
   "source": [
    "t10yie_data['Date'] = pd.to_datetime(t10yie_data['DATE'])\n",
    "reaintratrearat10y_data['Date'] = pd.to_datetime(reaintratrearat10y_data['DATE'])\n",
    "unrate['Date'] = pd.to_datetime(unrate['DATE'])\n",
    "CCI['Date'] = pd.to_datetime(CCI['Date'])\n",
    "vix['Date'] = pd.to_datetime(vix['Date'])\n",
    "\n",
    "\n",
    "# Set the 'Date' column as the index for each DataFrame\n",
    "t10yie_data.set_index('Date', inplace=True)\n",
    "reaintratrearat10y_data.set_index('Date', inplace=True)\n",
    "unrate.set_index('Date', inplace=True)\n",
    "CCI.set_index('Date', inplace=True)\n",
    "vix.set_index('Date', inplace=True)\n",
    "\n",
    "\n",
    "# Merge the dataframes on the index, which is the 'Date' column\n",
    "merged_data = combined_factors.merge(t10yie_data, how='inner', left_index=True, right_index=True)\n",
    "merged_data = merged_data.merge(reaintratrearat10y_data, how='inner', left_index=True, right_index=True)\n",
    "merged_data = merged_data.merge(unrate, how='inner', left_index=True, right_index=True)\n",
    "merged_data = merged_data.merge(CCI, how='inner', left_index=True, right_index=True)\n",
    "merged_data = merged_data.merge(vix, how='inner', left_index=True, right_index=True)\n",
    "merged_data = merged_data.merge(predictors, how='inner', left_index=True, right_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "e2ceee13-588b-4cfd-9a9b-cde6ef6fa6f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mkt-RF</th>\n",
       "      <th>HML</th>\n",
       "      <th>SMB</th>\n",
       "      <th>RMW</th>\n",
       "      <th>CMA</th>\n",
       "      <th>RF</th>\n",
       "      <th>Index_x</th>\n",
       "      <th>Dividend</th>\n",
       "      <th>Rfree</th>\n",
       "      <th>Inflation</th>\n",
       "      <th>InterestRate</th>\n",
       "      <th>UNRATE</th>\n",
       "      <th>CCI</th>\n",
       "      <th>VIX</th>\n",
       "      <th>D12</th>\n",
       "      <th>E12</th>\n",
       "      <th>svar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2003-04-01</th>\n",
       "      <td>8.22</td>\n",
       "      <td>1.14</td>\n",
       "      <td>1.02</td>\n",
       "      <td>-4.68</td>\n",
       "      <td>1.06</td>\n",
       "      <td>0.10</td>\n",
       "      <td>890.030000</td>\n",
       "      <td>16.203333</td>\n",
       "      <td>0.000926</td>\n",
       "      <td>1.82</td>\n",
       "      <td>1.309618</td>\n",
       "      <td>6.0</td>\n",
       "      <td>102.3</td>\n",
       "      <td>21.209999</td>\n",
       "      <td>16.204333</td>\n",
       "      <td>31.730000</td>\n",
       "      <td>0.003065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003-05-01</th>\n",
       "      <td>6.05</td>\n",
       "      <td>0.40</td>\n",
       "      <td>4.81</td>\n",
       "      <td>-7.00</td>\n",
       "      <td>2.91</td>\n",
       "      <td>0.09</td>\n",
       "      <td>935.960000</td>\n",
       "      <td>16.186667</td>\n",
       "      <td>0.000926</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1.370721</td>\n",
       "      <td>6.1</td>\n",
       "      <td>102.3</td>\n",
       "      <td>19.469999</td>\n",
       "      <td>16.184667</td>\n",
       "      <td>33.140000</td>\n",
       "      <td>0.002228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003-07-01</th>\n",
       "      <td>2.35</td>\n",
       "      <td>-1.24</td>\n",
       "      <td>4.54</td>\n",
       "      <td>-4.14</td>\n",
       "      <td>1.78</td>\n",
       "      <td>0.07</td>\n",
       "      <td>992.540000</td>\n",
       "      <td>16.310000</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>1.64</td>\n",
       "      <td>1.128689</td>\n",
       "      <td>6.2</td>\n",
       "      <td>102.3</td>\n",
       "      <td>19.490000</td>\n",
       "      <td>16.305333</td>\n",
       "      <td>35.893333</td>\n",
       "      <td>0.002046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003-08-01</th>\n",
       "      <td>2.34</td>\n",
       "      <td>1.53</td>\n",
       "      <td>2.48</td>\n",
       "      <td>-2.25</td>\n",
       "      <td>2.14</td>\n",
       "      <td>0.07</td>\n",
       "      <td>989.530000</td>\n",
       "      <td>16.450000</td>\n",
       "      <td>0.000823</td>\n",
       "      <td>2.04</td>\n",
       "      <td>1.751704</td>\n",
       "      <td>6.1</td>\n",
       "      <td>102.3</td>\n",
       "      <td>18.629999</td>\n",
       "      <td>16.445667</td>\n",
       "      <td>37.236667</td>\n",
       "      <td>0.000966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003-09-01</th>\n",
       "      <td>-1.24</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.54</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.08</td>\n",
       "      <td>1019.440000</td>\n",
       "      <td>16.590000</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>.</td>\n",
       "      <td>1.897238</td>\n",
       "      <td>6.1</td>\n",
       "      <td>102.4</td>\n",
       "      <td>22.719999</td>\n",
       "      <td>16.586000</td>\n",
       "      <td>38.580000</td>\n",
       "      <td>0.001869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-01</th>\n",
       "      <td>-6.94</td>\n",
       "      <td>-2.37</td>\n",
       "      <td>-1.59</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>1.77</td>\n",
       "      <td>0.21</td>\n",
       "      <td>2854.710000</td>\n",
       "      <td>55.698183</td>\n",
       "      <td>0.001934</td>\n",
       "      <td>1.93</td>\n",
       "      <td>0.540429</td>\n",
       "      <td>3.6</td>\n",
       "      <td>100.9</td>\n",
       "      <td>18.709999</td>\n",
       "      <td>55.697154</td>\n",
       "      <td>134.976667</td>\n",
       "      <td>0.001832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-01</th>\n",
       "      <td>1.19</td>\n",
       "      <td>0.48</td>\n",
       "      <td>-1.78</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.19</td>\n",
       "      <td>2996.113636</td>\n",
       "      <td>56.458183</td>\n",
       "      <td>0.001727</td>\n",
       "      <td>1.69</td>\n",
       "      <td>0.440572</td>\n",
       "      <td>3.7</td>\n",
       "      <td>100.4</td>\n",
       "      <td>16.120001</td>\n",
       "      <td>56.458019</td>\n",
       "      <td>134.480000</td>\n",
       "      <td>0.000594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-01</th>\n",
       "      <td>-2.58</td>\n",
       "      <td>-4.78</td>\n",
       "      <td>-3.24</td>\n",
       "      <td>0.56</td>\n",
       "      <td>-0.68</td>\n",
       "      <td>0.16</td>\n",
       "      <td>2897.498182</td>\n",
       "      <td>56.839092</td>\n",
       "      <td>0.001606</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.420052</td>\n",
       "      <td>3.6</td>\n",
       "      <td>100.1</td>\n",
       "      <td>18.980000</td>\n",
       "      <td>56.838763</td>\n",
       "      <td>133.690000</td>\n",
       "      <td>0.004318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-01</th>\n",
       "      <td>2.06</td>\n",
       "      <td>-1.91</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>-0.96</td>\n",
       "      <td>0.16</td>\n",
       "      <td>2977.680000</td>\n",
       "      <td>57.560000</td>\n",
       "      <td>0.001358</td>\n",
       "      <td>1.52</td>\n",
       "      <td>0.368325</td>\n",
       "      <td>3.6</td>\n",
       "      <td>100.4</td>\n",
       "      <td>13.220000</td>\n",
       "      <td>57.559879</td>\n",
       "      <td>135.090000</td>\n",
       "      <td>0.001510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-01</th>\n",
       "      <td>3.87</td>\n",
       "      <td>-2.02</td>\n",
       "      <td>0.44</td>\n",
       "      <td>-1.59</td>\n",
       "      <td>-1.24</td>\n",
       "      <td>0.12</td>\n",
       "      <td>3104.904500</td>\n",
       "      <td>57.900000</td>\n",
       "      <td>0.001272</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.433612</td>\n",
       "      <td>3.6</td>\n",
       "      <td>100.7</td>\n",
       "      <td>12.620000</td>\n",
       "      <td>57.900251</td>\n",
       "      <td>137.280000</td>\n",
       "      <td>0.000306</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>144 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Mkt-RF   HML   SMB   RMW   CMA    RF      Index_x   Dividend  \\\n",
       "2003-04-01    8.22  1.14  1.02 -4.68  1.06  0.10   890.030000  16.203333   \n",
       "2003-05-01    6.05  0.40  4.81 -7.00  2.91  0.09   935.960000  16.186667   \n",
       "2003-07-01    2.35 -1.24  4.54 -4.14  1.78  0.07   992.540000  16.310000   \n",
       "2003-08-01    2.34  1.53  2.48 -2.25  2.14  0.07   989.530000  16.450000   \n",
       "2003-09-01   -1.24  0.16  0.54  1.01  0.34  0.08  1019.440000  16.590000   \n",
       "...            ...   ...   ...   ...   ...   ...          ...        ...   \n",
       "2019-05-01   -6.94 -2.37 -1.59 -0.46  1.77  0.21  2854.710000  55.698183   \n",
       "2019-07-01    1.19  0.48 -1.78 -0.08  0.36  0.19  2996.113636  56.458183   \n",
       "2019-08-01   -2.58 -4.78 -3.24  0.56 -0.68  0.16  2897.498182  56.839092   \n",
       "2019-10-01    2.06 -1.91  0.26  0.44 -0.96  0.16  2977.680000  57.560000   \n",
       "2019-11-01    3.87 -2.02  0.44 -1.59 -1.24  0.12  3104.904500  57.900000   \n",
       "\n",
       "               Rfree Inflation  InterestRate  UNRATE    CCI        VIX  \\\n",
       "2003-04-01  0.000926      1.82      1.309618     6.0  102.3  21.209999   \n",
       "2003-05-01  0.000926      1.75      1.370721     6.1  102.3  19.469999   \n",
       "2003-07-01  0.000783      1.64      1.128689     6.2  102.3  19.490000   \n",
       "2003-08-01  0.000823      2.04      1.751704     6.1  102.3  18.629999   \n",
       "2003-09-01  0.000781         .      1.897238     6.1  102.4  22.719999   \n",
       "...              ...       ...           ...     ...    ...        ...   \n",
       "2019-05-01  0.001934      1.93      0.540429     3.6  100.9  18.709999   \n",
       "2019-07-01  0.001727      1.69      0.440572     3.7  100.4  16.120001   \n",
       "2019-08-01  0.001606       1.7      0.420052     3.6  100.1  18.980000   \n",
       "2019-10-01  0.001358      1.52      0.368325     3.6  100.4  13.220000   \n",
       "2019-11-01  0.001272      1.59      0.433612     3.6  100.7  12.620000   \n",
       "\n",
       "                  D12         E12      svar  \n",
       "2003-04-01  16.204333   31.730000  0.003065  \n",
       "2003-05-01  16.184667   33.140000  0.002228  \n",
       "2003-07-01  16.305333   35.893333  0.002046  \n",
       "2003-08-01  16.445667   37.236667  0.000966  \n",
       "2003-09-01  16.586000   38.580000  0.001869  \n",
       "...               ...         ...       ...  \n",
       "2019-05-01  55.697154  134.976667  0.001832  \n",
       "2019-07-01  56.458019  134.480000  0.000594  \n",
       "2019-08-01  56.838763  133.690000  0.004318  \n",
       "2019-10-01  57.559879  135.090000  0.001510  \n",
       "2019-11-01  57.900251  137.280000  0.000306  \n",
       "\n",
       "[144 rows x 17 columns]"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Drop the 'DATE_X' and 'DATE_Y' columns\n",
    "merged_data.drop(columns=['DATE_x', 'DATE_y','DATE','Index_y'], inplace=True)\n",
    "\n",
    "# Rename the columns\n",
    "merged_data.rename(columns={'T10YIE': 'Inflation', 'REAINTRATREARAT10Y': 'InterestRate'}, inplace=True)\n",
    "\n",
    "merged_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf78d88-b042-4c1e-acee-58d1a49ebbc7",
   "metadata": {},
   "source": [
    "#### Final Combined Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "6d1976bb-33f0-417b-b756-dfc2b1cd8c60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mkt-RF</th>\n",
       "      <th>HML</th>\n",
       "      <th>SMB</th>\n",
       "      <th>RMW</th>\n",
       "      <th>CMA</th>\n",
       "      <th>RF</th>\n",
       "      <th>Index_x</th>\n",
       "      <th>Dividend</th>\n",
       "      <th>Rfree</th>\n",
       "      <th>Inflation</th>\n",
       "      <th>InterestRate</th>\n",
       "      <th>UNRATE</th>\n",
       "      <th>CCI</th>\n",
       "      <th>VIX</th>\n",
       "      <th>D12</th>\n",
       "      <th>E12</th>\n",
       "      <th>svar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2003-04-01</th>\n",
       "      <td>8.22</td>\n",
       "      <td>1.14</td>\n",
       "      <td>1.02</td>\n",
       "      <td>-4.68</td>\n",
       "      <td>1.06</td>\n",
       "      <td>0.10</td>\n",
       "      <td>890.030000</td>\n",
       "      <td>16.203333</td>\n",
       "      <td>0.000926</td>\n",
       "      <td>1.82</td>\n",
       "      <td>1.309618</td>\n",
       "      <td>6.0</td>\n",
       "      <td>102.3</td>\n",
       "      <td>21.209999</td>\n",
       "      <td>16.204333</td>\n",
       "      <td>31.730000</td>\n",
       "      <td>0.003065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003-05-01</th>\n",
       "      <td>6.05</td>\n",
       "      <td>0.40</td>\n",
       "      <td>4.81</td>\n",
       "      <td>-7.00</td>\n",
       "      <td>2.91</td>\n",
       "      <td>0.09</td>\n",
       "      <td>935.960000</td>\n",
       "      <td>16.186667</td>\n",
       "      <td>0.000926</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1.370721</td>\n",
       "      <td>6.1</td>\n",
       "      <td>102.3</td>\n",
       "      <td>19.469999</td>\n",
       "      <td>16.184667</td>\n",
       "      <td>33.140000</td>\n",
       "      <td>0.002228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003-07-01</th>\n",
       "      <td>2.35</td>\n",
       "      <td>-1.24</td>\n",
       "      <td>4.54</td>\n",
       "      <td>-4.14</td>\n",
       "      <td>1.78</td>\n",
       "      <td>0.07</td>\n",
       "      <td>992.540000</td>\n",
       "      <td>16.310000</td>\n",
       "      <td>0.000783</td>\n",
       "      <td>1.64</td>\n",
       "      <td>1.128689</td>\n",
       "      <td>6.2</td>\n",
       "      <td>102.3</td>\n",
       "      <td>19.490000</td>\n",
       "      <td>16.305333</td>\n",
       "      <td>35.893333</td>\n",
       "      <td>0.002046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003-08-01</th>\n",
       "      <td>2.34</td>\n",
       "      <td>1.53</td>\n",
       "      <td>2.48</td>\n",
       "      <td>-2.25</td>\n",
       "      <td>2.14</td>\n",
       "      <td>0.07</td>\n",
       "      <td>989.530000</td>\n",
       "      <td>16.450000</td>\n",
       "      <td>0.000823</td>\n",
       "      <td>2.04</td>\n",
       "      <td>1.751704</td>\n",
       "      <td>6.1</td>\n",
       "      <td>102.3</td>\n",
       "      <td>18.629999</td>\n",
       "      <td>16.445667</td>\n",
       "      <td>37.236667</td>\n",
       "      <td>0.000966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2003-09-01</th>\n",
       "      <td>-1.24</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.54</td>\n",
       "      <td>1.01</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.08</td>\n",
       "      <td>1019.440000</td>\n",
       "      <td>16.590000</td>\n",
       "      <td>0.000781</td>\n",
       "      <td>.</td>\n",
       "      <td>1.897238</td>\n",
       "      <td>6.1</td>\n",
       "      <td>102.4</td>\n",
       "      <td>22.719999</td>\n",
       "      <td>16.586000</td>\n",
       "      <td>38.580000</td>\n",
       "      <td>0.001869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-01</th>\n",
       "      <td>-6.94</td>\n",
       "      <td>-2.37</td>\n",
       "      <td>-1.59</td>\n",
       "      <td>-0.46</td>\n",
       "      <td>1.77</td>\n",
       "      <td>0.21</td>\n",
       "      <td>2854.710000</td>\n",
       "      <td>55.698183</td>\n",
       "      <td>0.001934</td>\n",
       "      <td>1.93</td>\n",
       "      <td>0.540429</td>\n",
       "      <td>3.6</td>\n",
       "      <td>100.9</td>\n",
       "      <td>18.709999</td>\n",
       "      <td>55.697154</td>\n",
       "      <td>134.976667</td>\n",
       "      <td>0.001832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-01</th>\n",
       "      <td>1.19</td>\n",
       "      <td>0.48</td>\n",
       "      <td>-1.78</td>\n",
       "      <td>-0.08</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.19</td>\n",
       "      <td>2996.113636</td>\n",
       "      <td>56.458183</td>\n",
       "      <td>0.001727</td>\n",
       "      <td>1.69</td>\n",
       "      <td>0.440572</td>\n",
       "      <td>3.7</td>\n",
       "      <td>100.4</td>\n",
       "      <td>16.120001</td>\n",
       "      <td>56.458019</td>\n",
       "      <td>134.480000</td>\n",
       "      <td>0.000594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-01</th>\n",
       "      <td>-2.58</td>\n",
       "      <td>-4.78</td>\n",
       "      <td>-3.24</td>\n",
       "      <td>0.56</td>\n",
       "      <td>-0.68</td>\n",
       "      <td>0.16</td>\n",
       "      <td>2897.498182</td>\n",
       "      <td>56.839092</td>\n",
       "      <td>0.001606</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.420052</td>\n",
       "      <td>3.6</td>\n",
       "      <td>100.1</td>\n",
       "      <td>18.980000</td>\n",
       "      <td>56.838763</td>\n",
       "      <td>133.690000</td>\n",
       "      <td>0.004318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-01</th>\n",
       "      <td>2.06</td>\n",
       "      <td>-1.91</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.44</td>\n",
       "      <td>-0.96</td>\n",
       "      <td>0.16</td>\n",
       "      <td>2977.680000</td>\n",
       "      <td>57.560000</td>\n",
       "      <td>0.001358</td>\n",
       "      <td>1.52</td>\n",
       "      <td>0.368325</td>\n",
       "      <td>3.6</td>\n",
       "      <td>100.4</td>\n",
       "      <td>13.220000</td>\n",
       "      <td>57.559879</td>\n",
       "      <td>135.090000</td>\n",
       "      <td>0.001510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-01</th>\n",
       "      <td>3.87</td>\n",
       "      <td>-2.02</td>\n",
       "      <td>0.44</td>\n",
       "      <td>-1.59</td>\n",
       "      <td>-1.24</td>\n",
       "      <td>0.12</td>\n",
       "      <td>3104.904500</td>\n",
       "      <td>57.900000</td>\n",
       "      <td>0.001272</td>\n",
       "      <td>1.59</td>\n",
       "      <td>0.433612</td>\n",
       "      <td>3.6</td>\n",
       "      <td>100.7</td>\n",
       "      <td>12.620000</td>\n",
       "      <td>57.900251</td>\n",
       "      <td>137.280000</td>\n",
       "      <td>0.000306</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>144 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Mkt-RF   HML   SMB   RMW   CMA    RF      Index_x   Dividend  \\\n",
       "2003-04-01    8.22  1.14  1.02 -4.68  1.06  0.10   890.030000  16.203333   \n",
       "2003-05-01    6.05  0.40  4.81 -7.00  2.91  0.09   935.960000  16.186667   \n",
       "2003-07-01    2.35 -1.24  4.54 -4.14  1.78  0.07   992.540000  16.310000   \n",
       "2003-08-01    2.34  1.53  2.48 -2.25  2.14  0.07   989.530000  16.450000   \n",
       "2003-09-01   -1.24  0.16  0.54  1.01  0.34  0.08  1019.440000  16.590000   \n",
       "...            ...   ...   ...   ...   ...   ...          ...        ...   \n",
       "2019-05-01   -6.94 -2.37 -1.59 -0.46  1.77  0.21  2854.710000  55.698183   \n",
       "2019-07-01    1.19  0.48 -1.78 -0.08  0.36  0.19  2996.113636  56.458183   \n",
       "2019-08-01   -2.58 -4.78 -3.24  0.56 -0.68  0.16  2897.498182  56.839092   \n",
       "2019-10-01    2.06 -1.91  0.26  0.44 -0.96  0.16  2977.680000  57.560000   \n",
       "2019-11-01    3.87 -2.02  0.44 -1.59 -1.24  0.12  3104.904500  57.900000   \n",
       "\n",
       "               Rfree Inflation  InterestRate  UNRATE    CCI        VIX  \\\n",
       "2003-04-01  0.000926      1.82      1.309618     6.0  102.3  21.209999   \n",
       "2003-05-01  0.000926      1.75      1.370721     6.1  102.3  19.469999   \n",
       "2003-07-01  0.000783      1.64      1.128689     6.2  102.3  19.490000   \n",
       "2003-08-01  0.000823      2.04      1.751704     6.1  102.3  18.629999   \n",
       "2003-09-01  0.000781         .      1.897238     6.1  102.4  22.719999   \n",
       "...              ...       ...           ...     ...    ...        ...   \n",
       "2019-05-01  0.001934      1.93      0.540429     3.6  100.9  18.709999   \n",
       "2019-07-01  0.001727      1.69      0.440572     3.7  100.4  16.120001   \n",
       "2019-08-01  0.001606       1.7      0.420052     3.6  100.1  18.980000   \n",
       "2019-10-01  0.001358      1.52      0.368325     3.6  100.4  13.220000   \n",
       "2019-11-01  0.001272      1.59      0.433612     3.6  100.7  12.620000   \n",
       "\n",
       "                  D12         E12      svar  \n",
       "2003-04-01  16.204333   31.730000  0.003065  \n",
       "2003-05-01  16.184667   33.140000  0.002228  \n",
       "2003-07-01  16.305333   35.893333  0.002046  \n",
       "2003-08-01  16.445667   37.236667  0.000966  \n",
       "2003-09-01  16.586000   38.580000  0.001869  \n",
       "...               ...         ...       ...  \n",
       "2019-05-01  55.697154  134.976667  0.001832  \n",
       "2019-07-01  56.458019  134.480000  0.000594  \n",
       "2019-08-01  56.838763  133.690000  0.004318  \n",
       "2019-10-01  57.559879  135.090000  0.001510  \n",
       "2019-11-01  57.900251  137.280000  0.000306  \n",
       "\n",
       "[144 rows x 17 columns]"
      ]
     },
     "execution_count": 276,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f247b1f-537f-4ceb-9656-7ec9d342b78e",
   "metadata": {},
   "source": [
    "## Initial Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "e6093a66-8695-415f-b25c-735c24b207f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = merged_data.apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Drop any rows with NaN values that could have resulted from the conversion\n",
    "merged_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4f01d9-1550-4c90-881d-22dbf96852ab",
   "metadata": {},
   "source": [
    "### Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "574fa417-04e7-463b-83f0-fc104da32a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                 Mkt-RF   R-squared:                       0.575\n",
      "Model:                            OLS   Adj. R-squared:                  0.535\n",
      "Method:                 Least Squares   F-statistic:                     14.48\n",
      "Date:                Fri, 26 Apr 2024   Prob (F-statistic):           2.42e-17\n",
      "Time:                        11:48:17   Log-Likelihood:                -298.75\n",
      "No. Observations:                 130   AIC:                             621.5\n",
      "Df Residuals:                     118   BIC:                             655.9\n",
      "Df Model:                          11                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "================================================================================\n",
      "                   coef    std err          t      P>|t|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------\n",
      "const          100.8862     34.721      2.906      0.004      32.129     169.644\n",
      "Index_x          0.0036      0.003      1.066      0.289      -0.003       0.010\n",
      "Dividend        17.2252     98.677      0.175      0.862    -178.181     212.632\n",
      "Rfree          255.0021    459.714      0.555      0.580    -655.356    1165.360\n",
      "Inflation       -5.8306      1.053     -5.535      0.000      -7.917      -3.745\n",
      "InterestRate     0.7971      1.023      0.779      0.438      -1.229       2.824\n",
      "UNRATE           0.8559      0.274      3.122      0.002       0.313       1.399\n",
      "CCI             -0.8211      0.332     -2.472      0.015      -1.479      -0.163\n",
      "VIX             -0.3979      0.067     -5.946      0.000      -0.530      -0.265\n",
      "D12            -17.3741     98.695     -0.176      0.861    -212.818     178.069\n",
      "E12             -0.0610      0.028     -2.170      0.032      -0.117      -0.005\n",
      "svar          -129.9781     72.924     -1.782      0.077    -274.387      14.431\n",
      "==============================================================================\n",
      "Omnibus:                        2.519   Durbin-Watson:                   1.974\n",
      "Prob(Omnibus):                  0.284   Jarque-Bera (JB):                2.058\n",
      "Skew:                           0.292   Prob(JB):                        0.357\n",
      "Kurtosis:                       3.198   Cond. No.                     3.65e+06\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 3.65e+06. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "# Define the response variable 'MKT-RF' and the predictor variables (all other columns)\n",
    "X = merged_data.drop(columns=['Mkt-RF', 'HML', 'SMB', 'RMW', 'CMA', 'RF'])  # This assumes 'MKT-RF' is the only column to exclude from the predictors\n",
    "y = merged_data['Mkt-RF']\n",
    "\n",
    "# Add a constant to the predictor variables\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit the OLS regression model\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Print out the regression summary\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "8b193668-9f28-463c-b1f9-e46ed196a53a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                    HML   R-squared:                       0.149\n",
      "Model:                            OLS   Adj. R-squared:                  0.069\n",
      "Method:                 Least Squares   F-statistic:                     1.871\n",
      "Date:                Fri, 26 Apr 2024   Prob (F-statistic):             0.0499\n",
      "Time:                        11:48:19   Log-Likelihood:                -278.03\n",
      "No. Observations:                 130   AIC:                             580.1\n",
      "Df Residuals:                     118   BIC:                             614.5\n",
      "Df Model:                          11                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "================================================================================\n",
      "                   coef    std err          t      P>|t|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------\n",
      "const           -1.0633     29.607     -0.036      0.971     -59.692      57.566\n",
      "Index_x         -0.0031      0.003     -1.083      0.281      -0.009       0.003\n",
      "Dividend        56.5580     84.141      0.672      0.503    -110.064     223.180\n",
      "Rfree           18.7361    391.994      0.048      0.962    -757.519     794.991\n",
      "Inflation        1.1450      0.898      1.275      0.205      -0.634       2.924\n",
      "InterestRate    -0.6675      0.873     -0.765      0.446      -2.395       1.060\n",
      "UNRATE          -0.1259      0.234     -0.538      0.591      -0.589       0.337\n",
      "CCI              0.0410      0.283      0.145      0.885      -0.520       0.602\n",
      "VIX             -0.0805      0.057     -1.411      0.161      -0.194       0.032\n",
      "D12            -56.3988     84.157     -0.670      0.504    -223.052     110.254\n",
      "E12             -0.0315      0.024     -1.315      0.191      -0.079       0.016\n",
      "svar           -18.4223     62.182     -0.296      0.768    -141.559     104.714\n",
      "==============================================================================\n",
      "Omnibus:                        6.797   Durbin-Watson:                   2.084\n",
      "Prob(Omnibus):                  0.033   Jarque-Bera (JB):                6.654\n",
      "Skew:                           0.431   Prob(JB):                       0.0359\n",
      "Kurtosis:                       3.696   Cond. No.                     3.65e+06\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 3.65e+06. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "# Define the response variable 'HML' and the predictor variables (all other columns)\n",
    "X = merged_data.drop(columns=['Mkt-RF', 'HML', 'SMB', 'RMW', 'CMA', 'RF'])  # This assumes 'MKT-RF' is the only column to exclude from the predictors\n",
    "y = merged_data['HML']\n",
    "\n",
    "# Add a constant to the predictor variables\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit the OLS regression model\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Print out the regression summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "cffdefcd-b9d6-4cbd-9775-85b1c04599c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                    SMB   R-squared:                       0.198\n",
      "Model:                            OLS   Adj. R-squared:                  0.123\n",
      "Method:                 Least Squares   F-statistic:                     2.649\n",
      "Date:                Fri, 26 Apr 2024   Prob (F-statistic):            0.00458\n",
      "Time:                        11:48:23   Log-Likelihood:                -289.67\n",
      "No. Observations:                 130   AIC:                             603.3\n",
      "Df Residuals:                     118   BIC:                             637.7\n",
      "Df Model:                          11                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "================================================================================\n",
      "                   coef    std err          t      P>|t|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------\n",
      "const           12.2115     32.378      0.377      0.707     -51.905      76.328\n",
      "Index_x          0.0034      0.003      1.064      0.290      -0.003       0.010\n",
      "Dividend      -111.2878     92.016     -1.209      0.229    -293.505      70.930\n",
      "Rfree          435.9372    428.684      1.017      0.311    -412.974    1284.849\n",
      "Inflation       -2.0544      0.982     -2.091      0.039      -4.000      -0.109\n",
      "InterestRate    -0.5779      0.954     -0.606      0.546      -2.468       1.312\n",
      "UNRATE           0.3219      0.256      1.259      0.210      -0.184       0.828\n",
      "CCI             -0.0471      0.310     -0.152      0.879      -0.660       0.566\n",
      "VIX             -0.1112      0.062     -1.781      0.077      -0.235       0.012\n",
      "D12            111.1613     92.034      1.208      0.230     -71.090     293.413\n",
      "E12             -0.0514      0.026     -1.961      0.052      -0.103       0.001\n",
      "svar           -13.3088     68.002     -0.196      0.845    -147.971     121.353\n",
      "==============================================================================\n",
      "Omnibus:                        2.511   Durbin-Watson:                   2.142\n",
      "Prob(Omnibus):                  0.285   Jarque-Bera (JB):                2.397\n",
      "Skew:                           0.020   Prob(JB):                        0.302\n",
      "Kurtosis:                       3.664   Cond. No.                     3.65e+06\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 3.65e+06. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "# Define the response variable 'SMB' and the predictor variables (all other columns)\n",
    "X = merged_data.drop(columns=['Mkt-RF', 'HML', 'SMB', 'RMW', 'CMA', 'RF'])  # This assumes 'MKT-RF' is the only column to exclude from the predictors\n",
    "y = merged_data['SMB']\n",
    "\n",
    "# Add a constant to the predictor variables\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit the OLS regression model\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Print out the regression summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "3a673d40-4e2f-455b-9a98-f92d3c411511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                    RMW   R-squared:                       0.181\n",
      "Model:                            OLS   Adj. R-squared:                  0.104\n",
      "Method:                 Least Squares   F-statistic:                     2.367\n",
      "Date:                Fri, 26 Apr 2024   Prob (F-statistic):             0.0112\n",
      "Time:                        11:48:24   Log-Likelihood:                -240.97\n",
      "No. Observations:                 130   AIC:                             505.9\n",
      "Df Residuals:                     118   BIC:                             540.4\n",
      "Df Model:                          11                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "================================================================================\n",
      "                   coef    std err          t      P>|t|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------\n",
      "const          -28.1145     22.262     -1.263      0.209     -72.200      15.971\n",
      "Index_x         -0.0047      0.002     -2.144      0.034      -0.009      -0.000\n",
      "Dividend       -59.8386     63.269     -0.946      0.346    -185.129      65.452\n",
      "Rfree         -383.7846    294.758     -1.302      0.195    -967.485     199.916\n",
      "Inflation        1.6524      0.675      2.446      0.016       0.315       2.990\n",
      "InterestRate     0.9028      0.656      1.376      0.171      -0.396       2.202\n",
      "UNRATE           0.0039      0.176      0.022      0.982      -0.344       0.352\n",
      "CCI              0.2052      0.213      0.964      0.337      -0.216       0.627\n",
      "VIX              0.0739      0.043      1.723      0.087      -0.011       0.159\n",
      "D12             60.0693     63.281      0.949      0.344     -65.245     185.383\n",
      "E12              0.0324      0.018      1.799      0.075      -0.003       0.068\n",
      "svar            -4.7546     46.757     -0.102      0.919     -97.347      87.837\n",
      "==============================================================================\n",
      "Omnibus:                        5.279   Durbin-Watson:                   1.533\n",
      "Prob(Omnibus):                  0.071   Jarque-Bera (JB):                7.792\n",
      "Skew:                           0.018   Prob(JB):                       0.0203\n",
      "Kurtosis:                       4.199   Cond. No.                     3.65e+06\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 3.65e+06. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "# Define the response variable 'RMW' and the predictor variables (all other columns)\n",
    "X = merged_data.drop(columns=['Mkt-RF', 'HML', 'SMB', 'RMW', 'CMA', 'RF'])  # This assumes 'MKT-RF' is the only column to exclude from the predictors\n",
    "y = merged_data['RMW']\n",
    "\n",
    "# Add a constant to the predictor variables\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit the OLS regression model\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Print out the regression summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "a1200de1-63c0-4e53-8e96-7207199b6a74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                    CMA   R-squared:                       0.088\n",
      "Model:                            OLS   Adj. R-squared:                  0.003\n",
      "Method:                 Least Squares   F-statistic:                     1.035\n",
      "Date:                Fri, 26 Apr 2024   Prob (F-statistic):              0.420\n",
      "Time:                        11:48:26   Log-Likelihood:                -223.31\n",
      "No. Observations:                 130   AIC:                             470.6\n",
      "Df Residuals:                     118   BIC:                             505.0\n",
      "Df Model:                          11                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "================================================================================\n",
      "                   coef    std err          t      P>|t|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------\n",
      "const           -3.5910     19.435     -0.185      0.854     -42.078      34.895\n",
      "Index_x         -0.0011      0.002     -0.586      0.559      -0.005       0.003\n",
      "Dividend        85.2974     55.233      1.544      0.125     -24.080     194.675\n",
      "Rfree          152.9376    257.321      0.594      0.553    -356.628     662.503\n",
      "Inflation       -0.1356      0.590     -0.230      0.819      -1.303       1.032\n",
      "InterestRate    -0.2789      0.573     -0.487      0.627      -1.413       0.855\n",
      "UNRATE           0.1118      0.153      0.729      0.468      -0.192       0.416\n",
      "CCI              0.0415      0.186      0.223      0.824      -0.327       0.410\n",
      "VIX             -0.0184      0.037     -0.492      0.624      -0.093       0.056\n",
      "D12            -85.2725     55.244     -1.544      0.125    -194.670      24.125\n",
      "E12              0.0065      0.016      0.411      0.682      -0.025       0.038\n",
      "svar            27.5248     40.819      0.674      0.501     -53.307     108.357\n",
      "==============================================================================\n",
      "Omnibus:                        2.601   Durbin-Watson:                   1.935\n",
      "Prob(Omnibus):                  0.272   Jarque-Bera (JB):                2.562\n",
      "Skew:                           0.337   Prob(JB):                        0.278\n",
      "Kurtosis:                       2.863   Cond. No.                     3.65e+06\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 3.65e+06. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n"
     ]
    }
   ],
   "source": [
    "# Define the response variable 'CMA' and the predictor variables (all other columns)\n",
    "X = merged_data.drop(columns=['Mkt-RF', 'HML', 'SMB', 'RMW', 'CMA', 'RF'])  # This assumes 'MKT-RF' is the only column to exclude from the predictors\n",
    "y = merged_data['CMA']\n",
    "\n",
    "# Add a constant to the predictor variables\n",
    "X = sm.add_constant(X)\n",
    "\n",
    "# Fit the OLS regression model\n",
    "model = sm.OLS(y, X).fit()\n",
    "\n",
    "# Print out the regression summary\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c3843f-a516-4163-a5fc-ce2cbde2a237",
   "metadata": {},
   "source": [
    "### Forward Stepwise Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "61922880-fc92-42d0-b9d0-95a8985a6349",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Regression Summary for Factor: Mkt-RF\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                 Mkt-RF   R-squared:                       0.570\n",
      "Model:                            OLS   Adj. R-squared:                  0.542\n",
      "Method:                 Least Squares   F-statistic:                     20.06\n",
      "Date:                Fri, 26 Apr 2024   Prob (F-statistic):           5.14e-19\n",
      "Time:                        14:42:35   Log-Likelihood:                -299.42\n",
      "No. Observations:                 130   AIC:                             616.8\n",
      "Df Residuals:                     121   BIC:                             642.6\n",
      "Df Model:                           8                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "================================================================================\n",
      "                   coef    std err          t      P>|t|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------\n",
      "const           87.1848     29.347      2.971      0.004      29.084     145.285\n",
      "svar          -155.5782     68.234     -2.280      0.024    -290.665     -20.492\n",
      "Inflation       -5.7750      1.023     -5.644      0.000      -7.801      -3.749\n",
      "D12             -0.0103      0.052     -0.199      0.843      -0.113       0.093\n",
      "VIX             -0.3951      0.066     -5.988      0.000      -0.526      -0.264\n",
      "UNRATE           0.7689      0.247      3.110      0.002       0.279       1.258\n",
      "E12             -0.0373      0.018     -2.094      0.038      -0.073      -0.002\n",
      "CCI             -0.6872      0.278     -2.476      0.015      -1.237      -0.138\n",
      "InterestRate     1.3174      0.667      1.974      0.051      -0.004       2.639\n",
      "==============================================================================\n",
      "Omnibus:                        2.184   Durbin-Watson:                   1.959\n",
      "Prob(Omnibus):                  0.335   Jarque-Bera (JB):                1.808\n",
      "Skew:                           0.282   Prob(JB):                        0.405\n",
      "Kurtosis:                       3.127   Cond. No.                     4.21e+04\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 4.21e+04. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "\n",
      "Selected Predictors: ['svar', 'Inflation', 'D12', 'VIX', 'UNRATE', 'E12', 'CCI', 'InterestRate']\n",
      "--------------------------------------------------------------------------------\n",
      "Regression Summary for Factor: SMB\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                    SMB   R-squared:                       0.165\n",
      "Model:                            OLS   Adj. R-squared:                  0.138\n",
      "Method:                 Least Squares   F-statistic:                     6.155\n",
      "Date:                Fri, 26 Apr 2024   Prob (F-statistic):           0.000149\n",
      "Time:                        14:42:35   Log-Likelihood:                -292.33\n",
      "No. Observations:                 130   AIC:                             594.7\n",
      "Df Residuals:                     125   BIC:                             609.0\n",
      "Df Model:                           4                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const          7.8124      1.973      3.959      0.000       3.907      11.718\n",
      "E12           -0.0334      0.008     -4.123      0.000      -0.049      -0.017\n",
      "svar         -62.9339     57.986     -1.085      0.280    -177.696      51.828\n",
      "Inflation     -1.5728      0.579     -2.714      0.008      -2.720      -0.426\n",
      "VIX           -0.0738      0.051     -1.438      0.153      -0.175       0.028\n",
      "==============================================================================\n",
      "Omnibus:                        0.970   Durbin-Watson:                   2.082\n",
      "Prob(Omnibus):                  0.616   Jarque-Bera (JB):                0.549\n",
      "Skew:                          -0.001   Prob(JB):                        0.760\n",
      "Kurtosis:                       3.318   Cond. No.                     2.45e+04\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 2.45e+04. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "\n",
      "Selected Predictors: ['E12', 'svar', 'Inflation', 'VIX']\n",
      "--------------------------------------------------------------------------------\n",
      "Regression Summary for Factor: HML\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                    HML   R-squared:                       0.120\n",
      "Model:                            OLS   Adj. R-squared:                  0.099\n",
      "Method:                 Least Squares   F-statistic:                     5.718\n",
      "Date:                Fri, 26 Apr 2024   Prob (F-statistic):            0.00106\n",
      "Time:                        14:42:35   Log-Likelihood:                -280.19\n",
      "No. Observations:                 130   AIC:                             568.4\n",
      "Df Residuals:                     126   BIC:                             579.9\n",
      "Df Model:                           3                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "================================================================================\n",
      "                   coef    std err          t      P>|t|      [0.025      0.975]\n",
      "--------------------------------------------------------------------------------\n",
      "const            4.7376      1.236      3.832      0.000       2.291       7.184\n",
      "E12             -0.0323      0.008     -3.996      0.000      -0.048      -0.016\n",
      "VIX             -0.0916      0.031     -2.947      0.004      -0.153      -0.030\n",
      "InterestRate    -0.5244      0.309     -1.696      0.092      -1.136       0.087\n",
      "==============================================================================\n",
      "Omnibus:                        9.849   Durbin-Watson:                   2.048\n",
      "Prob(Omnibus):                  0.007   Jarque-Bera (JB):               11.120\n",
      "Skew:                           0.506   Prob(JB):                      0.00385\n",
      "Kurtosis:                       4.015   Cond. No.                         584.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n",
      "Selected Predictors: ['E12', 'VIX', 'InterestRate']\n",
      "--------------------------------------------------------------------------------\n",
      "Regression Summary for Factor: RMW\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                    RMW   R-squared:                       0.136\n",
      "Model:                            OLS   Adj. R-squared:                  0.108\n",
      "Method:                 Least Squares   F-statistic:                     4.910\n",
      "Date:                Fri, 26 Apr 2024   Prob (F-statistic):            0.00104\n",
      "Time:                        14:42:35   Log-Likelihood:                -244.45\n",
      "No. Observations:                 130   AIC:                             498.9\n",
      "Df Residuals:                     125   BIC:                             513.2\n",
      "Df Model:                           4                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -6.2068      1.719     -3.610      0.000      -9.609      -2.804\n",
      "svar          17.8921     40.376      0.443      0.658     -62.018      97.802\n",
      "Inflation      1.6466      0.485      3.392      0.001       0.686       2.607\n",
      "D12            0.0422      0.015      2.753      0.007       0.012       0.073\n",
      "VIX            0.0859      0.036      2.398      0.018       0.015       0.157\n",
      "==============================================================================\n",
      "Omnibus:                        8.460   Durbin-Watson:                   1.416\n",
      "Prob(Omnibus):                  0.015   Jarque-Bera (JB):               16.813\n",
      "Skew:                          -0.110   Prob(JB):                     0.000223\n",
      "Kurtosis:                       4.748   Cond. No.                     1.09e+04\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "[2] The condition number is large, 1.09e+04. This might indicate that there are\n",
      "strong multicollinearity or other numerical problems.\n",
      "\n",
      "Selected Predictors: ['svar', 'Inflation', 'D12', 'VIX']\n",
      "--------------------------------------------------------------------------------\n",
      "Regression Summary for Factor: CMA\n",
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                    CMA   R-squared:                       0.036\n",
      "Model:                            OLS   Adj. R-squared:                  0.028\n",
      "Method:                 Least Squares   F-statistic:                     4.776\n",
      "Date:                Fri, 26 Apr 2024   Prob (F-statistic):             0.0307\n",
      "Time:                        14:42:35   Log-Likelihood:                -226.92\n",
      "No. Observations:                 130   AIC:                             457.8\n",
      "Df Residuals:                     128   BIC:                             463.6\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "const         -0.7509      0.405     -1.855      0.066      -1.552       0.050\n",
      "UNRATE         0.1383      0.063      2.185      0.031       0.013       0.263\n",
      "==============================================================================\n",
      "Omnibus:                        1.666   Durbin-Watson:                   1.829\n",
      "Prob(Omnibus):                  0.435   Jarque-Bera (JB):                1.713\n",
      "Skew:                           0.261   Prob(JB):                        0.425\n",
      "Kurtosis:                       2.791   Cond. No.                         21.6\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
      "\n",
      "Selected Predictors: ['UNRATE']\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "import itertools\n",
    "\n",
    "# Define the list of factors you want to test\n",
    "factors = ['Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA']\n",
    "\n",
    "# Define all possible predictors, excluding the factors and any non-predictor columns\n",
    "predictors = ['Inflation', 'InterestRate', 'UNRATE', 'CCI', 'VIX', 'D12', 'E12', 'svar']  # adjust based on actual predictor columns\n",
    "\n",
    "# Initialize a dictionary to store the selected predictors for each factor\n",
    "final_model_selection = {}\n",
    "\n",
    "# Perform forward selection for each factor\n",
    "for factor in factors:\n",
    "    y = merged_data[factor]  # Dependent variable\n",
    "    selected_predictors = []\n",
    "    remaining_predictors = list(predictors)\n",
    "    best_AIC = np.inf\n",
    "    \n",
    "    while remaining_predictors:\n",
    "        scores_with_candidates = []\n",
    "        for predictor in remaining_predictors:\n",
    "            trial_predictors = selected_predictors + [predictor]\n",
    "            X = sm.add_constant(merged_data[trial_predictors])\n",
    "            model = sm.OLS(y, X).fit()\n",
    "            scores_with_candidates.append((model.aic, predictor))\n",
    "        \n",
    "        # Sort the candidates based on AIC\n",
    "        scores_with_candidates.sort()\n",
    "        best_new_score, best_new_predictor = scores_with_candidates[0]\n",
    "        \n",
    "        # If the new score is lower than the current best, update the selected predictors\n",
    "        if best_new_score < best_AIC:\n",
    "            best_AIC = best_new_score\n",
    "            remaining_predictors.remove(best_new_predictor)\n",
    "            selected_predictors.append(best_new_predictor)\n",
    "        else:\n",
    "            # If no improvement, stop the process for the current factor\n",
    "            break\n",
    "\n",
    "    # Store the final selected predictors in the dictionary\n",
    "    final_model_selection[factor] = selected_predictors\n",
    "\n",
    "    # Print the summary of the final model for the current factor\n",
    "    X_final = sm.add_constant(merged_data[selected_predictors])\n",
    "    final_model = sm.OLS(y, X_final).fit()\n",
    "    print(f'Regression Summary for Factor: {factor}')\n",
    "    print(final_model.summary())\n",
    "    print(\"\\nSelected Predictors:\", selected_predictors)\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "73c42ac6-f0c4-4f3f-823d-1a91e751154a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out-of-Sample R^2 for Mkt-RF: -0.7585\n",
      "Out-of-Sample R^2 for SMB: 0.0128\n",
      "Out-of-Sample R^2 for HML: 0.0967\n",
      "Out-of-Sample R^2 for RMW: -0.4001\n",
      "Out-of-Sample R^2 for CMA: 0.0170\n",
      "Combined Out-of-Sample R^2 across all factors: -0.2064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/8v/pfws4r5s7rx80y_f5_qhv_5r0000gn/T/ipykernel_1454/3402109985.py:3: FutureWarning: Passing method to DatetimeIndex.get_loc is deprecated and will raise in a future version. Use index.get_indexer([item], method=...) instead.\n",
      "  ind_aux = merged_data.index.get_loc(oos_start_date, method='backfill')\n"
     ]
    }
   ],
   "source": [
    "# Define the start date for the out-of-sample period\n",
    "oos_start_date = pd.Timestamp('2016-01-01')\n",
    "ind_aux = merged_data.index.get_loc(oos_start_date, method='backfill')\n",
    "\n",
    "# Calculate out-of-sample R-squared for each factor\n",
    "R2_oos_factors = {}\n",
    "\n",
    "for factor in factors:\n",
    "    in_sample_data = merged_data.iloc[:ind_aux]\n",
    "    out_of_sample_data = merged_data.iloc[ind_aux:]\n",
    "\n",
    "    X_in = sm.add_constant(in_sample_data[final_model_selection[factor]])\n",
    "    y_in = in_sample_data[factor]\n",
    "    model = sm.OLS(y_in, X_in).fit()\n",
    "\n",
    "    X_out = sm.add_constant(out_of_sample_data[final_model_selection[factor]])\n",
    "    y_out = out_of_sample_data[factor]\n",
    "    predictions = model.predict(X_out)\n",
    "\n",
    "    ss_res = np.sum((y_out - predictions) ** 2)\n",
    "    ss_tot = np.sum((y_out - np.mean(y_out)) ** 2)\n",
    "    R2_oos_factors[factor] = 1 - ss_res / ss_tot\n",
    "    print(f'Out-of-Sample R^2 for {factor}: {R2_oos_factors[factor]:.4f}')\n",
    "\n",
    "# Combined performance metric\n",
    "R2_oos_combined = np.mean(list(R2_oos_factors.values()))\n",
    "print(f'Combined Out-of-Sample R^2 across all factors: {R2_oos_combined:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbb62f9-5184-4bb6-b3b1-bf920ff3df36",
   "metadata": {},
   "source": [
    "## Testing Other ML Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853f9386-3956-4e37-908f-93bfc65e4dc2",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "ac21dc16-e9fc-4a3d-8c20-d05251f8b52b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for Mkt-RF model:\n",
      "Cross-Validation Scores: [-0.05160713 -0.97098336 -0.49641089  0.43610056 -2.32960774]\n",
      "Mean Squared Error: 7.617411538461538\n",
      "Out-of-Sample R^2: 0.19823402447058613\n",
      "\n",
      "\n",
      "Metrics for SMB model:\n",
      "Cross-Validation Scores: [-0.0837674  -4.61620685 -0.78953196 -2.71504674 -0.43448961]\n",
      "Mean Squared Error: 12.738784615384615\n",
      "Out-of-Sample R^2: -0.7284583901839252\n",
      "\n",
      "\n",
      "Metrics for HML model:\n",
      "Cross-Validation Scores: [-4.89175794 -0.17045361 -0.47557763  0.04852716 -1.72669774]\n",
      "Mean Squared Error: 10.503984615384617\n",
      "Out-of-Sample R^2: -0.22380369707040382\n",
      "\n",
      "\n",
      "Metrics for RMW model:\n",
      "Cross-Validation Scores: [-0.97007481 -1.94547188 -1.52353239 -0.39136101 -0.57582097]\n",
      "Mean Squared Error: 3.9011038461538465\n",
      "Out-of-Sample R^2: -1.62335469967288\n",
      "\n",
      "\n",
      "Metrics for CMA model:\n",
      "Cross-Validation Scores: [-1.2140047  -1.27007558 -3.09643611 -1.69061721 -0.98003103]\n",
      "Mean Squared Error: 5.47641923076923\n",
      "Out-of-Sample R^2: -0.9526342417138858\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assume 'df' is your dataframe with all the data\n",
    "predictors = ['Inflation', 'InterestRate', 'UNRATE', 'CCI', 'VIX', 'D12', 'E12', 'svar']\n",
    "factors = ['Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X = merged_data[predictors]\n",
    "y = merged_data[factors]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Initialize a dictionary to store the models and their metrics\n",
    "models_metrics = {}\n",
    "\n",
    "# Loop through each factor to create models\n",
    "for factor in factors:\n",
    "    # Train the model\n",
    "    dt_model = DecisionTreeRegressor(random_state=0)\n",
    "    dt_model.fit(X_train, y_train[factor])\n",
    "    \n",
    "    # Evaluate using cross-validation on the training set\n",
    "    cv_scores = cross_val_score(dt_model, X_train, y_train[factor], cv=5, scoring='r2')\n",
    "    \n",
    "    # Predict on the out-of-sample data\n",
    "    predictions = dt_model.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy metrics\n",
    "    mse = mean_squared_error(y_test[factor], predictions)\n",
    "    r2 = r2_score(y_test[factor], predictions)\n",
    "    \n",
    "    # Store metrics\n",
    "    models_metrics[factor] = {\n",
    "        'Cross-Validation Scores': cv_scores,\n",
    "        'Mean Squared Error': mse,\n",
    "        'Out-of-Sample R^2': r2,\n",
    "        # Calculate additional metrics as needed here\n",
    "    }\n",
    "\n",
    "# Output the models' metrics\n",
    "for factor, metrics in models_metrics.items():\n",
    "    print(f\"Metrics for {factor} model:\")\n",
    "    for metric_name, metric_value in metrics.items():\n",
    "        print(f\"{metric_name}: {metric_value}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ada5842-23ce-4ede-be49-8182bc798ee2",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "26109ec7-6854-435d-824e-4154fb489348",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metrics for Mkt-RF model:\n",
      "Cross-Validation Scores: [ 0.10588005  0.20214861 -0.18992257  0.38766657 -1.706742  ]\n",
      "Mean Squared Error: 7.294305859615386\n",
      "Out-of-Sample R^2: 0.2322423143589778\n",
      "\n",
      "\n",
      "Metrics for SMB model:\n",
      "Cross-Validation Scores: [ 0.12012852 -0.42318331 -0.39064376 -1.10700011 -0.09406779]\n",
      "Mean Squared Error: 6.916472826538461\n",
      "Out-of-Sample R^2: 0.06154033932941361\n",
      "\n",
      "\n",
      "Metrics for HML model:\n",
      "Cross-Validation Scores: [-0.17274945 -0.13290097 -0.14981836  0.08658851 -0.00294902]\n",
      "Mean Squared Error: 9.302155248846155\n",
      "Out-of-Sample R^2: -0.08378033680544839\n",
      "\n",
      "\n",
      "Metrics for RMW model:\n",
      "Cross-Validation Scores: [-0.26169866 -0.80991943 -1.16471365  0.14316347  0.25519173]\n",
      "Mean Squared Error: 2.0497432580769233\n",
      "Out-of-Sample R^2: -0.3783800230030685\n",
      "\n",
      "\n",
      "Metrics for CMA model:\n",
      "Cross-Validation Scores: [-0.09393154 -0.35281131 -0.64982504 -0.05899938 -0.0248161 ]\n",
      "Mean Squared Error: 2.723979106923077\n",
      "Out-of-Sample R^2: 0.028756811018629813\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "predictors = ['Inflation', 'InterestRate', 'UNRATE', 'CCI', 'VIX', 'D12', 'E12', 'svar']\n",
    "factors = ['Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X = merged_data[predictors]\n",
    "y = merged_data[factors]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Initialize a dictionary to store the models and their metrics\n",
    "models_metrics = {}\n",
    "\n",
    "# Loop through each factor to create models\n",
    "for factor in factors:\n",
    "    # Train the model\n",
    "    rf_model = RandomForestRegressor(random_state=0, n_estimators=100)\n",
    "    rf_model.fit(X_train, y_train[factor])\n",
    "    \n",
    "    # Evaluate using cross-validation on the training set\n",
    "    cv_scores = cross_val_score(rf_model, X_train, y_train[factor], cv=5, scoring='r2')\n",
    "    \n",
    "    # Predict on the out-of-sample data\n",
    "    predictions = rf_model.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy metrics\n",
    "    mse = mean_squared_error(y_test[factor], predictions)\n",
    "    r2 = r2_score(y_test[factor], predictions)\n",
    "    \n",
    "    # Store metrics\n",
    "    models_metrics[factor] = {\n",
    "        'Cross-Validation Scores': cv_scores,\n",
    "        'Mean Squared Error': mse,\n",
    "        'Out-of-Sample R^2': r2,\n",
    "        # Calculate additional metrics as needed here\n",
    "    }\n",
    "\n",
    "# Output the models' metrics\n",
    "for factor, metrics in models_metrics.items():\n",
    "    print(f\"Metrics for {factor} model:\")\n",
    "    for metric_name, metric_value in metrics.items():\n",
    "        print(f\"{metric_name}: {metric_value}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf7c7c0-ff60-4ee5-aef6-cb2e2ee80783",
   "metadata": {},
   "source": [
    "#### Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "9d86af38-05b7-4b2f-951a-25090aed6ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Define predictors and factors\n",
    "predictors = ['Inflation', 'InterestRate', 'UNRATE', 'CCI', 'VIX', 'D12', 'E12', 'svar']\n",
    "factors = ['Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA']\n",
    "\n",
    "# Scale the predictor variables\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(merged_data[predictors])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, merged_data[factors], test_size=0.2, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "34429757-37c4-4504-a94c-8afe7d13eafc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/keras/src/layers/core/dense.py:88: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# Define a simple sequential model\n",
    "def build_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Dense(64, activation='relu', input_shape=(input_shape,)),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(len(factors))  # Output layer: Predicting all factors simultaneously\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "model = build_model(X_train.shape[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "d8a5fb18-e0f2-4ed7-bc23-d06918d60002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 6.8899 - mae: 1.9110 - val_loss: 5.9554 - val_mae: 1.8268\n",
      "Epoch 2/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 6.6529 - mae: 1.8599 - val_loss: 5.8920 - val_mae: 1.8218\n",
      "Epoch 3/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 6.3955 - mae: 1.8070 - val_loss: 5.8359 - val_mae: 1.8168\n",
      "Epoch 4/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 6.1433 - mae: 1.7727 - val_loss: 5.7818 - val_mae: 1.8113\n",
      "Epoch 5/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5.9122 - mae: 1.7626 - val_loss: 5.7314 - val_mae: 1.8076\n",
      "Epoch 6/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5.7365 - mae: 1.7439 - val_loss: 5.6855 - val_mae: 1.8057\n",
      "Epoch 7/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 6.0269 - mae: 1.7765 - val_loss: 5.6426 - val_mae: 1.8037\n",
      "Epoch 8/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5.5850 - mae: 1.6985 - val_loss: 5.5992 - val_mae: 1.8014\n",
      "Epoch 9/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5.6844 - mae: 1.7141 - val_loss: 5.5608 - val_mae: 1.7995\n",
      "Epoch 10/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5.5903 - mae: 1.6857 - val_loss: 5.5229 - val_mae: 1.7975\n",
      "Epoch 11/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5.4997 - mae: 1.6722 - val_loss: 5.4880 - val_mae: 1.7966\n",
      "Epoch 12/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5.7500 - mae: 1.7050 - val_loss: 5.4577 - val_mae: 1.7987\n",
      "Epoch 13/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5.5535 - mae: 1.7018 - val_loss: 5.4327 - val_mae: 1.8016\n",
      "Epoch 14/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 5.1148 - mae: 1.6510 - val_loss: 5.4112 - val_mae: 1.8043\n",
      "Epoch 15/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.7565 - mae: 1.5955 - val_loss: 5.3861 - val_mae: 1.8064\n",
      "Epoch 16/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 5.2882 - mae: 1.6988 - val_loss: 5.3616 - val_mae: 1.8086\n",
      "Epoch 17/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 4.8608 - mae: 1.6282 - val_loss: 5.3369 - val_mae: 1.8108\n",
      "Epoch 18/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.3637 - mae: 1.5240 - val_loss: 5.3181 - val_mae: 1.8129\n",
      "Epoch 19/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.4121 - mae: 1.5533 - val_loss: 5.3009 - val_mae: 1.8165\n",
      "Epoch 20/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.6580 - mae: 1.5913 - val_loss: 5.2891 - val_mae: 1.8223\n",
      "Epoch 21/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.5973 - mae: 1.5863 - val_loss: 5.2724 - val_mae: 1.8270\n",
      "Epoch 22/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.6057 - mae: 1.5646 - val_loss: 5.2635 - val_mae: 1.8325\n",
      "Epoch 23/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.2895 - mae: 1.5430 - val_loss: 5.2636 - val_mae: 1.8387\n",
      "Epoch 24/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.0268 - mae: 1.5012 - val_loss: 5.2725 - val_mae: 1.8465\n",
      "Epoch 25/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 4.2417 - mae: 1.5378 - val_loss: 5.2904 - val_mae: 1.8559\n",
      "Epoch 26/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.6391 - mae: 1.4279 - val_loss: 5.3073 - val_mae: 1.8637\n",
      "Epoch 27/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.9117 - mae: 1.4722 - val_loss: 5.3270 - val_mae: 1.8713\n",
      "Epoch 28/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.7243 - mae: 1.4531 - val_loss: 5.3601 - val_mae: 1.8806\n",
      "Epoch 29/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.9731 - mae: 1.5063 - val_loss: 5.3869 - val_mae: 1.8882\n",
      "Epoch 30/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.6965 - mae: 1.4297 - val_loss: 5.4231 - val_mae: 1.8963\n",
      "Epoch 31/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.7766 - mae: 1.4508 - val_loss: 5.4507 - val_mae: 1.9033\n",
      "Epoch 32/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.4332 - mae: 1.3903 - val_loss: 5.4949 - val_mae: 1.9134\n",
      "Epoch 33/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.7396 - mae: 1.4511 - val_loss: 5.5337 - val_mae: 1.9229\n",
      "Epoch 34/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.6658 - mae: 1.4451 - val_loss: 5.5789 - val_mae: 1.9326\n",
      "Epoch 35/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.4607 - mae: 1.3936 - val_loss: 5.6216 - val_mae: 1.9411\n",
      "Epoch 36/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.5571 - mae: 1.4210 - val_loss: 5.6449 - val_mae: 1.9460\n",
      "Epoch 37/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.2293 - mae: 1.3440 - val_loss: 5.6872 - val_mae: 1.9536\n",
      "Epoch 38/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.1523 - mae: 1.3395 - val_loss: 5.7349 - val_mae: 1.9617\n",
      "Epoch 39/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.5246 - mae: 1.4152 - val_loss: 5.7550 - val_mae: 1.9650\n",
      "Epoch 40/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.7455 - mae: 1.4550 - val_loss: 5.7810 - val_mae: 1.9716\n",
      "Epoch 41/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.2069 - mae: 1.3177 - val_loss: 5.8151 - val_mae: 1.9801\n",
      "Epoch 42/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 3.3945 - mae: 1.3862 - val_loss: 5.8617 - val_mae: 1.9904\n",
      "Epoch 43/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2.9759 - mae: 1.2998 - val_loss: 5.8901 - val_mae: 1.9969\n",
      "Epoch 44/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.8375 - mae: 1.2597 - val_loss: 5.9279 - val_mae: 2.0050\n",
      "Epoch 45/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.2436 - mae: 1.3371 - val_loss: 5.9360 - val_mae: 2.0070\n",
      "Epoch 46/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.3726 - mae: 1.3906 - val_loss: 5.9526 - val_mae: 2.0109\n",
      "Epoch 47/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.0103 - mae: 1.3002 - val_loss: 5.9753 - val_mae: 2.0159\n",
      "Epoch 48/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.1343 - mae: 1.3323 - val_loss: 5.9879 - val_mae: 2.0187\n",
      "Epoch 49/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.9364 - mae: 1.2771 - val_loss: 6.0130 - val_mae: 2.0239\n",
      "Epoch 50/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.3626 - mae: 1.3574 - val_loss: 6.0051 - val_mae: 2.0227\n",
      "Epoch 51/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.9610 - mae: 1.2797 - val_loss: 6.0314 - val_mae: 2.0271\n",
      "Epoch 52/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.1250 - mae: 1.3141 - val_loss: 6.0310 - val_mae: 2.0265\n",
      "Epoch 53/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.1497 - mae: 1.3242 - val_loss: 6.0224 - val_mae: 2.0249\n",
      "Epoch 54/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.8967 - mae: 1.2852 - val_loss: 6.0360 - val_mae: 2.0273\n",
      "Epoch 55/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.1342 - mae: 1.2965 - val_loss: 6.0684 - val_mae: 2.0324\n",
      "Epoch 56/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.7454 - mae: 1.2336 - val_loss: 6.0641 - val_mae: 2.0315\n",
      "Epoch 57/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.8775 - mae: 1.2726 - val_loss: 6.0724 - val_mae: 2.0324\n",
      "Epoch 58/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.1264 - mae: 1.3127 - val_loss: 6.0832 - val_mae: 2.0339\n",
      "Epoch 59/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.0790 - mae: 1.3174 - val_loss: 6.0856 - val_mae: 2.0341\n",
      "Epoch 60/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.4374 - mae: 1.3885 - val_loss: 6.1027 - val_mae: 2.0363\n",
      "Epoch 61/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.9629 - mae: 1.2910 - val_loss: 6.0880 - val_mae: 2.0329\n",
      "Epoch 62/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.8559 - mae: 1.2514 - val_loss: 6.0944 - val_mae: 2.0335\n",
      "Epoch 63/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.7081 - mae: 1.2427 - val_loss: 6.1127 - val_mae: 2.0373\n",
      "Epoch 64/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.8443 - mae: 1.2544 - val_loss: 6.1179 - val_mae: 2.0383\n",
      "Epoch 65/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.6689 - mae: 1.2350 - val_loss: 6.1186 - val_mae: 2.0369\n",
      "Epoch 66/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.7898 - mae: 1.2522 - val_loss: 6.1053 - val_mae: 2.0335\n",
      "Epoch 67/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.6411 - mae: 1.2132 - val_loss: 6.0869 - val_mae: 2.0301\n",
      "Epoch 68/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.0084 - mae: 1.2838 - val_loss: 6.0675 - val_mae: 2.0257\n",
      "Epoch 69/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.0492 - mae: 1.2908 - val_loss: 6.0588 - val_mae: 2.0224\n",
      "Epoch 70/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.6564 - mae: 1.2258 - val_loss: 6.0768 - val_mae: 2.0241\n",
      "Epoch 71/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.0345 - mae: 1.3116 - val_loss: 6.0836 - val_mae: 2.0261\n",
      "Epoch 72/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.6929 - mae: 1.2171 - val_loss: 6.1005 - val_mae: 2.0283\n",
      "Epoch 73/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.6915 - mae: 1.2510 - val_loss: 6.0996 - val_mae: 2.0278\n",
      "Epoch 74/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 3.1032 - mae: 1.3122 - val_loss: 6.1161 - val_mae: 2.0299\n",
      "Epoch 75/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.8276 - mae: 1.2556 - val_loss: 6.0968 - val_mae: 2.0258\n",
      "Epoch 76/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.6884 - mae: 1.2408 - val_loss: 6.0639 - val_mae: 2.0195\n",
      "Epoch 77/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.6701 - mae: 1.2291 - val_loss: 6.0434 - val_mae: 2.0140\n",
      "Epoch 78/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.7775 - mae: 1.2507 - val_loss: 6.0309 - val_mae: 2.0106\n",
      "Epoch 79/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.7507 - mae: 1.2472 - val_loss: 6.0609 - val_mae: 2.0157\n",
      "Epoch 80/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.6482 - mae: 1.2402 - val_loss: 6.0922 - val_mae: 2.0215\n",
      "Epoch 81/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 3.1304 - mae: 1.3327 - val_loss: 6.0892 - val_mae: 2.0204\n",
      "Epoch 82/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.6068 - mae: 1.2200 - val_loss: 6.0971 - val_mae: 2.0189\n",
      "Epoch 83/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.4184 - mae: 1.1705 - val_loss: 6.0732 - val_mae: 2.0132\n",
      "Epoch 84/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.9057 - mae: 1.2736 - val_loss: 6.0636 - val_mae: 2.0128\n",
      "Epoch 85/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.7887 - mae: 1.2608 - val_loss: 6.0605 - val_mae: 2.0126\n",
      "Epoch 86/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.6679 - mae: 1.2212 - val_loss: 6.0704 - val_mae: 2.0132\n",
      "Epoch 87/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.7229 - mae: 1.2431 - val_loss: 6.1018 - val_mae: 2.0191\n",
      "Epoch 88/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.7928 - mae: 1.2633 - val_loss: 6.0957 - val_mae: 2.0158\n",
      "Epoch 89/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step - loss: 2.6726 - mae: 1.2323 - val_loss: 6.0868 - val_mae: 2.0126\n",
      "Epoch 90/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2.5155 - mae: 1.2080 - val_loss: 6.0969 - val_mae: 2.0130\n",
      "Epoch 91/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2.4687 - mae: 1.1754 - val_loss: 6.0849 - val_mae: 2.0102\n",
      "Epoch 92/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.7840 - mae: 1.2500 - val_loss: 6.1296 - val_mae: 2.0199\n",
      "Epoch 93/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.3308 - mae: 1.1501 - val_loss: 6.1309 - val_mae: 2.0198\n",
      "Epoch 94/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.6399 - mae: 1.2227 - val_loss: 6.1253 - val_mae: 2.0181\n",
      "Epoch 95/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.5274 - mae: 1.1838 - val_loss: 6.1401 - val_mae: 2.0191\n",
      "Epoch 96/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.7210 - mae: 1.2294 - val_loss: 6.1521 - val_mae: 2.0191\n",
      "Epoch 97/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2.6411 - mae: 1.2140 - val_loss: 6.1425 - val_mae: 2.0166\n",
      "Epoch 98/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.5819 - mae: 1.2035 - val_loss: 6.1252 - val_mae: 2.0135\n",
      "Epoch 99/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.4623 - mae: 1.1875 - val_loss: 6.1356 - val_mae: 2.0148\n",
      "Epoch 100/100\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 2.5468 - mae: 1.2174 - val_loss: 6.1576 - val_mae: 2.0172\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, epochs=100, validation_split=0.1, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "388b0789-2173-45e3-8cc6-e579019270d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 5.9284 - mae: 1.9070\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "R2 Scores: {'Mkt-RF': 0.08035290497634429, 'SMB': -0.09079609929054033, 'HML': 0.10443613943290264, 'RMW': -0.5541706693105484, 'CMA': -0.02245119796363637}\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test data\n",
    "test_results = model.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "# Predictions for further performance metrics calculation\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Calculate R-squared for each factor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "r2_scores = {factor: r2_score(y_test[factor], predictions[:, i]) for i, factor in enumerate(factors)}\n",
    "print(\"R2 Scores:\", r2_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "f814ffd0-c791-4b43-a6d1-568a902f3f3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38484c43-01dc-4a2c-835f-a8eed711bd9f",
   "metadata": {},
   "source": [
    "Training Loss (Blue Line): The loss on the training set decreases consistently as the number of epochs increases. This indicates that the model is learning from the training data, fitting the training dataset better over time. Validation Loss (Orange Line): The validation loss decreases initially and then plateaus, maintaining a consistent level of loss across epochs. This behavior is typical and suggests that the model has learned to generalize to some extent, as it is not showing signs of overfitting with an increasing validation loss. Gap Between Training and Validation Loss: The gap between the training and validation loss seems relatively stable after the initial epochs. This is a good sign as it implies that the model has not overfit the training data; otherwise, we would see the training loss continuing to decrease while the validation loss started to increase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19f08a3-438c-428b-9af2-1111395d46a0",
   "metadata": {},
   "source": [
    "For Mkt-RF, the Random Forest model has the highest out-of-sample 𝑅2, indicating the best performance among the models tested.\n",
    "\n",
    "For SMB, while still negative, the Random Forest again has the least negative out-of-sample 𝑅2, which suggests it's the best (least bad) among the models tested.\n",
    "\n",
    "For HML, the Neural Network model outperforms the other models with a positive out-of-sample 𝑅2.\n",
    "\n",
    "For RMW, the Neural Network model has the least negative out-of-sample 𝑅2, making it the relative best among the models for this factor.\n",
    "\n",
    "For CMA, the Random Forest model provides a small positive out-of-sample 𝑅2, though it's close to the performance of the Neural Network.\n",
    "\n",
    "Based on these observations, you might consider using a combination of models for different factors. The Random Forest seems to be the most consistently strong performer across multiple factors, with the Neural Network model showing promise particularly for HML and RMW."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a21a26f4-a8be-4730-bc31-8f71c1886b08",
   "metadata": {},
   "source": [
    "## Trading Strategy : Factor Timing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc95b7c-ec66-48ee-90d7-0ab957688b3c",
   "metadata": {},
   "source": [
    "### Calculating Metrics for each Factors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee333cf2-5ba3-49e1-b4a8-5678e08ec83c",
   "metadata": {},
   "source": [
    "The code below makes predictions for several factors without implementing a trading strategy. This is just to set a benchmark as to what the our predictions are going to look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "2d747c95-76c9-4b1d-9d8a-ca8180c8c7ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Annualized Returns  Annualized StDev  Max Drawdown  Sharpe Ratio\n",
      "Mkt-RF           17.465492          3.244433     -0.279100      5.382706\n",
      "SMB              -2.514323          1.372590     -0.999990     -1.833023\n",
      "HML               2.517185          2.529168     -0.996554      0.994603\n",
      "RMW              -0.771646          2.005076     -0.999697     -0.385678\n",
      "CMA              -1.195200          0.976175     -0.993360     -1.226077\n"
     ]
    }
   ],
   "source": [
    "# The code below summarizes Annualized Returns, Annualized StDev, Max Drawdown, and Sharpe Ratio of each factor \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# Define your predictors and factors\n",
    "predictors = ['Inflation', 'InterestRate', 'UNRATE', 'CCI', 'VIX', 'D12', 'E12', 'svar']\n",
    "factors = ['Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "# For time-series, it's important not to shuffle the dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(merged_data[predictors], merged_data[factors], test_size=0.2, shuffle=False)\n",
    "\n",
    "# Initialize the Random Forest model\n",
    "rf_model = RandomForestRegressor(random_state=0, n_estimators=100)\n",
    "\n",
    "# Dictionary to hold predictions\n",
    "rf_predictions = {}\n",
    "\n",
    "# Train a model for each factor and make predictions\n",
    "for factor in factors:\n",
    "    rf_model.fit(X_train, y_train[factor])\n",
    "    predictions = rf_model.predict(X_test)\n",
    "    rf_predictions[factor] = predictions\n",
    "\n",
    "# Convert predictions to DataFrame\n",
    "rf_predictions_df = pd.DataFrame(rf_predictions, index=y_test.index)\n",
    "\n",
    "# Calculate annualized returns and stdev assuming monthly returns\n",
    "annualized_returns = rf_predictions_df.mean() * 12\n",
    "annualized_std = rf_predictions_df.std() * np.sqrt(12)\n",
    "\n",
    "# Function to calculate max drawdown\n",
    "def max_drawdown(return_series):\n",
    "    comp_ret = (1 + return_series).cumprod()\n",
    "    peak = comp_ret.expanding(min_periods=1).max()\n",
    "    drawdown = (comp_ret - peak) / peak\n",
    "    return drawdown.min()\n",
    "\n",
    "# Calculate max drawdown\n",
    "max_drawdowns = rf_predictions_df.apply(max_drawdown)\n",
    "\n",
    "# Risk-free rate for Sharpe Ratio\n",
    "risk_free_rate = 0.02 / 12  # assuming a 2% annual rate\n",
    "\n",
    "# Calculate Sharpe Ratio\n",
    "sharpe_ratios = (annualized_returns - risk_free_rate) / annualized_std\n",
    "\n",
    "# Output the metrics\n",
    "metrics = pd.DataFrame({\n",
    "    'Annualized Returns': annualized_returns,\n",
    "    'Annualized StDev': annualized_std,\n",
    "    'Max Drawdown': max_drawdowns,\n",
    "    'Sharpe Ratio': sharpe_ratios\n",
    "})\n",
    "\n",
    "print(metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8999f131-0c75-40f0-9018-552b180d9d09",
   "metadata": {},
   "source": [
    "## Running Random Forest on each factors separately"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f01d1ca-baf9-4599-8432-5ee987738029",
   "metadata": {},
   "source": [
    "#### HML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "c8ff123f-b734-4697-a1f0-25bcf2413721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Drawdown: -12.3438103037658\n",
      "Annualized Mean Returns: 4.172307692307692\n",
      "Annualized Standard Deviation: 10.530496547573698\n",
      "Sharpe Ratio: 0.3861750836006268\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'merged_data' is your DataFrame with predictors and the 'HML' factor\n",
    "predictors = ['Inflation', 'InterestRate', 'UNRATE', 'CCI', 'VIX', 'D12', 'E12', 'svar']\n",
    "X = merged_data[predictors]\n",
    "y = merged_data['HML']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Train the Random Forest model\n",
    "rf_model = RandomForestRegressor(random_state=0, n_estimators=100)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Generate predictions for the out-of-sample data\n",
    "predictions = rf_model.predict(X_test)\n",
    "\n",
    "# Create signals based on predictions and shift them to the next period to simulate real trading\n",
    "signals = np.where(predictions > 0, 1, -1)\n",
    "shifted_signals = np.roll(signals, shift=1)\n",
    "# The first signal can't be used because we don't have a previous prediction for it\n",
    "shifted_signals[0] = 0\n",
    "\n",
    "# Calculate strategy returns\n",
    "strategy_returns = y_test * shifted_signals\n",
    "\n",
    "# Calculate cumulative returns for the investment strategy\n",
    "cumulative_returns = (1 + strategy_returns).cumprod() - 1\n",
    "\n",
    "# Calculate max drawdown\n",
    "rolling_max = cumulative_returns.cummax()\n",
    "drawdowns = (cumulative_returns - rolling_max) / rolling_max\n",
    "max_drawdown = drawdowns.min()\n",
    "\n",
    "# Calculate other performance metrics\n",
    "average_monthly_rf_rate = merged_data['RF'].mean()  # Convert the mean RF to decimal if needed\n",
    "annualized_mean_returns = strategy_returns.mean() * 12\n",
    "annualized_std_dev = strategy_returns.std() * np.sqrt(12)\n",
    "sharpe_ratio = (annualized_mean_returns - average_monthly_rf_rate) / annualized_std_dev\n",
    "\n",
    "# Print the performance metrics\n",
    "print(f\"Max Drawdown: {max_drawdown}\")\n",
    "print(f\"Annualized Mean Returns: {annualized_mean_returns}\")\n",
    "print(f\"Annualized Standard Deviation: {annualized_std_dev}\")\n",
    "print(f\"Sharpe Ratio: {sharpe_ratio}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e7e62e-a118-4395-8a71-531d3e90629a",
   "metadata": {},
   "source": [
    "#### Mkt-RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "59f7eb1e-153d-47df-be06-e9ded57f5fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Drawdown: -213.9449491385011\n",
      "Annualized Mean Returns: 8.146153846153847\n",
      "Annualized Standard Deviation: 10.962358842175135\n",
      "Sharpe Ratio: 0.7334608959823251\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'merged_data' is a DataFrame with your predictors and the Mkt-RF factor returns\n",
    "predictors = ['Inflation', 'InterestRate', 'UNRATE', 'CCI', 'VIX', 'D12', 'E12', 'svar']\n",
    "X = merged_data[predictors]\n",
    "y = merged_data['Mkt-RF']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Random Forest model\n",
    "rf_model = RandomForestRegressor(random_state=0, n_estimators=100)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Generate predictions\n",
    "predictions = rf_model.predict(X_test)\n",
    "\n",
    "# Create signals and shift them for realistic trading\n",
    "signals = np.where(predictions > 0, 1, -1)\n",
    "shifted_signals = np.roll(signals, shift=1)\n",
    "shifted_signals[0] = 0  # First signal cannot be acted upon\n",
    "\n",
    "# Calculate strategy returns\n",
    "strategy_returns = y_test * shifted_signals\n",
    "\n",
    "# Convert the mean RF to decimal and use it directly (ensure it's monthly and convert to decimal if necessary)\n",
    "average_monthly_rf_rate = merged_data['RF'].mean()  # Assuming it's given in percentage form\n",
    "\n",
    "# Calculate performance metrics\n",
    "annualized_mean_returns = strategy_returns.mean() * 12\n",
    "annualized_std_dev = strategy_returns.std() * np.sqrt(12)\n",
    "sharpe_ratio = (annualized_mean_returns - average_monthly_rf_rate) / annualized_std_dev\n",
    "\n",
    "# Calculate cumulative strategy returns and max drawdown\n",
    "cumulative_returns = (1 + strategy_returns).cumprod() - 1\n",
    "rolling_max = np.maximum.accumulate(cumulative_returns)\n",
    "drawdowns = (cumulative_returns - rolling_max) / rolling_max\n",
    "max_drawdown = drawdowns.min()\n",
    "\n",
    "# Output the performance metrics\n",
    "print(f\"Max Drawdown: {max_drawdown}\")\n",
    "print(f\"Annualized Mean Returns: {annualized_mean_returns}\")\n",
    "print(f\"Annualized Standard Deviation: {annualized_std_dev}\")\n",
    "print(f\"Sharpe Ratio: {sharpe_ratio}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950ab636-bb3c-4749-9aea-db68b9ebeaa9",
   "metadata": {},
   "source": [
    "#### SMB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "06bb527d-6d81-4dc4-a9d3-887245f09f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Drawdown: -761.5337905117318\n",
      "Annualized Mean Returns: 4.6523076923076925\n",
      "Annualized Standard Deviation: 9.429105447414486\n",
      "Sharpe Ratio: 0.48218947279480184\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'merged_data' is a DataFrame with your predictors and the SMB factor returns\n",
    "predictors = ['Inflation', 'InterestRate', 'UNRATE', 'CCI', 'VIX', 'D12', 'E12', 'svar']\n",
    "X = merged_data[predictors]\n",
    "y = merged_data['SMB']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Random Forest model\n",
    "rf_model = RandomForestRegressor(random_state=0, n_estimators=100)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Generate predictions\n",
    "predictions = rf_model.predict(X_test)\n",
    "\n",
    "# Create signals and shift them for realistic trading\n",
    "signals = np.where(predictions > 0, 1, -1)\n",
    "shifted_signals = np.roll(signals, shift=1)\n",
    "shifted_signals[0] = 0  # First signal cannot be acted upon\n",
    "\n",
    "# Calculate strategy returns\n",
    "strategy_returns = y_test * shifted_signals\n",
    "\n",
    "# Use the direct average monthly risk-free rate (ensure it's in decimal form)\n",
    "average_monthly_rf_rate = merged_data['RF'].mean() # Convert from percentage if necessary\n",
    "\n",
    "# Calculate performance metrics\n",
    "annualized_mean_returns = strategy_returns.mean() * 12\n",
    "annualized_std_dev = strategy_returns.std() * np.sqrt(12)\n",
    "sharpe_ratio = (annualized_mean_returns - average_monthly_rf_rate) / annualized_std_dev\n",
    "\n",
    "# Calculate cumulative strategy returns and max drawdown\n",
    "cumulative_returns = (1 + strategy_returns).cumprod() - 1\n",
    "\n",
    "# Correcting the calculation for max drawdown to avoid division by zero or -inf values\n",
    "cumulative_returns[cumulative_returns == 0] = -0.0001\n",
    "rolling_max = np.maximum.accumulate(cumulative_returns)\n",
    "rolling_max[rolling_max == 0] = np.finfo(float).eps  # smallest positive float\n",
    "max_drawdown = ((cumulative_returns - rolling_max) / rolling_max).min()\n",
    "\n",
    "# Output the performance metrics\n",
    "print(f\"Max Drawdown: {max_drawdown}\")\n",
    "print(f\"Annualized Mean Returns: {annualized_mean_returns}\")\n",
    "print(f\"Annualized Standard Deviation: {annualized_std_dev}\")\n",
    "print(f\"Sharpe Ratio: {sharpe_ratio}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e67826-e04d-48b4-b022-e6711f9c9846",
   "metadata": {},
   "source": [
    "#### CMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "f748dec4-22b9-4324-b666-1efa47f79003",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Drawdown: -4.608685897620486\n",
      "Annualized Mean Returns: 0.6507692307692309\n",
      "Annualized Standard Deviation: 6.0111909481719685\n",
      "Sharpe Ratio: 0.09067702686148803\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming 'merged_data' is a DataFrame with your predictors and the CMA factor returns\n",
    "predictors = ['Inflation', 'InterestRate', 'UNRATE', 'CCI', 'VIX', 'D12', 'E12', 'svar']\n",
    "X = merged_data[predictors]\n",
    "y = merged_data['CMA']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Random Forest model\n",
    "rf_model = RandomForestRegressor(random_state=0, n_estimators=100)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Generate predictions\n",
    "predictions = rf_model.predict(X_test)\n",
    "\n",
    "# Create signals and shift them for realistic trading\n",
    "signals = np.where(predictions > 0, 1, -1)\n",
    "shifted_signals = np.roll(signals, shift=1)\n",
    "shifted_signals[0] = 0  # First signal cannot be acted upon\n",
    "\n",
    "# Calculate strategy returns\n",
    "strategy_returns = y_test * shifted_signals\n",
    "\n",
    "# Use the direct average monthly risk-free rate (ensure it's in decimal form)\n",
    "average_monthly_rf_rate = merged_data['RF'].mean()   # Convert from percentage if necessary\n",
    "\n",
    "# Calculate performance metrics\n",
    "annualized_mean_returns = strategy_returns.mean() * 12\n",
    "annualized_std_dev = strategy_returns.std() * np.sqrt(12)\n",
    "sharpe_ratio = (annualized_mean_returns - average_monthly_rf_rate) / annualized_std_dev\n",
    "\n",
    "# Calculate cumulative strategy returns and max drawdown\n",
    "cumulative_returns = (1 + strategy_returns).cumprod() - 1\n",
    "rolling_max = np.maximum.accumulate(cumulative_returns)\n",
    "max_drawdown = ((cumulative_returns - rolling_max) / rolling_max).min()\n",
    "\n",
    "# Output the performance metrics\n",
    "print(f\"Max Drawdown: {max_drawdown}\")\n",
    "print(f\"Annualized Mean Returns: {annualized_mean_returns}\")\n",
    "print(f\"Annualized Standard Deviation: {annualized_std_dev}\")\n",
    "print(f\"Sharpe Ratio: {sharpe_ratio}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92a55b6-7cef-442e-b2e7-5cb857aa4bde",
   "metadata": {},
   "source": [
    "#### RMW "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "850bc529-e243-4ed3-bb03-516549e70f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Drawdown: -0.9999999999999998\n",
      "Annualized Mean Returns: 0.6384\n",
      "Annualized Standard Deviation: 4.109436944400047\n",
      "Sharpe Ratio: 0.15509255539048675\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming 'merged_data' is your DataFrame with predictors and the RMW factor\n",
    "predictors = ['Inflation', 'InterestRate', 'UNRATE', 'CCI', 'VIX', 'D12', 'E12', 'svar']\n",
    "X = merged_data[predictors]\n",
    "y = merged_data['RMW']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Random Forest model\n",
    "rf_model = RandomForestRegressor(random_state=0, n_estimators=100)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Generate predictions\n",
    "predictions = rf_model.predict(X_test)\n",
    "\n",
    "# Create signals and shift them for realistic trading\n",
    "signals = np.where(predictions > 0, 1, -1)\n",
    "shifted_signals = np.roll(signals, shift=1)\n",
    "shifted_signals[0] = 0  # First signal cannot be acted upon\n",
    "\n",
    "# Align returns with shifted signals\n",
    "strategy_returns = y_test.shift(-1) * shifted_signals\n",
    "strategy_returns = strategy_returns.dropna()\n",
    "\n",
    "# Cumulative returns and max drawdown\n",
    "cumulative_returns = (1 + strategy_returns).cumprod() - 1\n",
    "cumulative_returns[cumulative_returns <= 0] = np.finfo(float).eps\n",
    "rolling_max = cumulative_returns.cummax()\n",
    "drawdowns = (cumulative_returns - rolling_max) / rolling_max\n",
    "max_drawdown = drawdowns.min()\n",
    "\n",
    "# Performance metrics\n",
    "average_monthly_rf_rate = merged_data['RF'].mean()\n",
    "annualized_mean_returns = strategy_returns.mean() * 12\n",
    "annualized_std_dev = strategy_returns.std() * np.sqrt(12)\n",
    "sharpe_ratio = (annualized_mean_returns - (average_monthly_rf_rate / 100)) / annualized_std_dev\n",
    "\n",
    "# Output the performance metrics\n",
    "print(f\"Max Drawdown: {max_drawdown}\")\n",
    "print(f\"Annualized Mean Returns: {annualized_mean_returns}\")\n",
    "print(f\"Annualized Standard Deviation: {annualized_std_dev}\")\n",
    "print(f\"Sharpe Ratio: {sharpe_ratio}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346fc84e-2bf3-4e5c-a797-ae97fb8248ad",
   "metadata": {},
   "source": [
    "## Transaction Costs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ae7c52-9992-4e82-b36b-c4e6ef56292a",
   "metadata": {},
   "source": [
    "The code below calculates transaction costs for each individual model listed above. Each model must be run before executing the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "9bcfa0e2-5e3a-4951-b954-f8b8ec3c4c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transaction Cost: 10 bps\n",
      "Max Drawdown: -0.9999999999999998\n",
      "Annualized Mean Returns: 0.632160000000001\n",
      "Annualized Standard Deviation: 4.109752634891789\n",
      "Sharpe Ratio: 0.1535623024035594\n",
      "\n",
      "Transaction Cost: 20 bps\n",
      "Max Drawdown: -0.9999999999999998\n",
      "Annualized Mean Returns: 0.6259200000000001\n",
      "Annualized Standard Deviation: 4.110069060247041\n",
      "Sharpe Ratio: 0.15203225730847422\n",
      "\n",
      "Transaction Cost: 50 bps\n",
      "Max Drawdown: -0.9999999999999998\n",
      "Annualized Mean Returns: 0.6072000000000002\n",
      "Annualized Standard Deviation: 4.1110227437950275\n",
      "Sharpe Ratio: 0.14744337715911673\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def calculate_strategy_metrics(transaction_cost_bps):\n",
    "    # Convert basis points to a decimal\n",
    "    transaction_cost = transaction_cost_bps / 10000\n",
    "    \n",
    "    # Calculate strategy returns with transaction cost\n",
    "    # Assume costs occur every time the position changes (i.e., when the signal changes)\n",
    "    # Calculate changes in position only for periods where strategy returns are available\n",
    "    changes_in_position = np.diff(shifted_signals[:len(strategy_returns)], prepend=0) != 0\n",
    "\n",
    "    net_returns = strategy_returns - changes_in_position * transaction_cost\n",
    "\n",
    "    # Calculate cumulative returns\n",
    "    cumulative_returns = (1 + net_returns).cumprod() - 1\n",
    "    cumulative_returns[cumulative_returns <= 0] = np.finfo(float).eps\n",
    "\n",
    "    # Max drawdown calculation\n",
    "    rolling_max = cumulative_returns.cummax()\n",
    "    drawdowns = (cumulative_returns - rolling_max) / rolling_max\n",
    "    max_drawdown = drawdowns.min()\n",
    "\n",
    "    # Performance metrics\n",
    "    annualized_mean_returns = net_returns.mean() * 12\n",
    "    annualized_std_dev = net_returns.std() * np.sqrt(12)\n",
    "    sharpe_ratio = (annualized_mean_returns - (average_monthly_rf_rate / 100)) / annualized_std_dev\n",
    "\n",
    "    # Output the performance metrics\n",
    "    print(f\"Transaction Cost: {transaction_cost_bps} bps\")\n",
    "    print(f\"Max Drawdown: {max_drawdown}\")\n",
    "    print(f\"Annualized Mean Returns: {annualized_mean_returns}\")\n",
    "    print(f\"Annualized Standard Deviation: {annualized_std_dev}\")\n",
    "    print(f\"Sharpe Ratio: {sharpe_ratio}\\n\")\n",
    "\n",
    "# Test various levels of transaction costs\n",
    "for cost in [10, 20, 50]:\n",
    "    calculate_strategy_metrics(cost)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e82b24-32a9-4a2a-ad09-65d5789eb15b",
   "metadata": {},
   "source": [
    "## Exploring Combo Factor Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b8f49a-3203-40e7-a726-a97a37ec96a0",
   "metadata": {},
   "source": [
    "#### Equal Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "32d85f19-d7cd-4ab0-b11d-ff681f5a9e5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Drawdown: -3.2454897452558455\n",
      "Annualized Mean Returns: 1.4335384615384614\n",
      "Annualized Standard Deviation: 4.769045210683\n",
      "Sharpe Ratio: 0.27843018784382334\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'merged_data' is your DataFrame with all predictors and factors\n",
    "predictors = ['Inflation', 'InterestRate', 'UNRATE', 'CCI', 'VIX', 'D12', 'E12', 'svar']\n",
    "factors = ['Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA']\n",
    "\n",
    "# Initialize Random Forest models for each factor\n",
    "rf_models = {factor: RandomForestRegressor(random_state=0, n_estimators=100) for factor in factors}\n",
    "\n",
    "# Train-test split and train models\n",
    "split_data = {}\n",
    "for factor in factors:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(merged_data[predictors], merged_data[factor], test_size=0.2, shuffle=False)\n",
    "    split_data[factor] = (X_train, X_test, y_train, y_test)\n",
    "    rf_models[factor].fit(X_train, y_train)\n",
    "\n",
    "# Generate predictions and signals for the out-of-sample data\n",
    "factor_predictions = {}\n",
    "for factor in factors:\n",
    "    _, X_test, _, y_test = split_data[factor]\n",
    "    predictions = rf_models[factor].predict(X_test)\n",
    "    factor_predictions[factor] = predictions\n",
    "\n",
    "# Create a DataFrame for predictions\n",
    "predictions_df = pd.DataFrame(factor_predictions, index=X_test.index)\n",
    "\n",
    "# Generate equal-weighted composite signals from the predictions\n",
    "composite_signals = np.sign(predictions_df.mean(axis=1))\n",
    "\n",
    "# Shift signals to avoid lookahead bias\n",
    "shifted_composite_signals = np.roll(composite_signals, shift=1)\n",
    "shifted_composite_signals[0] = 0  # First signal is unusable\n",
    "\n",
    "# Align the signals with the out-of-sample data\n",
    "aligned_signals = pd.Series(shifted_composite_signals, index=predictions_df.index)\n",
    "\n",
    "# Calculate strategy returns for each factor\n",
    "strategy_returns_df = pd.DataFrame(index=predictions_df.index)\n",
    "for factor in factors:\n",
    "    _, _, _, y_test = split_data[factor]\n",
    "    strategy_returns_df[factor] = aligned_signals * y_test\n",
    "\n",
    "# Portfolio strategy return is the mean return across all factors\n",
    "portfolio_strategy_returns = strategy_returns_df.mean(axis=1)\n",
    "\n",
    "# Performance Metrics Calculation\n",
    "cumulative_returns = (1 + portfolio_strategy_returns).cumprod() - 1\n",
    "rolling_max = cumulative_returns.cummax()\n",
    "drawdowns = (cumulative_returns - rolling_max) / rolling_max\n",
    "max_drawdown = drawdowns.min()\n",
    "\n",
    "average_monthly_rf_rate = merged_data['RF'].mean()  # Assuming RF column for risk-free rate\n",
    "annualized_mean_returns = portfolio_strategy_returns.mean() * 12\n",
    "annualized_std_dev = portfolio_strategy_returns.std() * np.sqrt(12)\n",
    "sharpe_ratio = (annualized_mean_returns - average_monthly_rf_rate) / annualized_std_dev\n",
    "\n",
    "# Output the metrics\n",
    "print(f\"Max Drawdown: {max_drawdown}\")\n",
    "print(f\"Annualized Mean Returns: {annualized_mean_returns}\")\n",
    "print(f\"Annualized Standard Deviation: {annualized_std_dev}\")\n",
    "print(f\"Sharpe Ratio: {sharpe_ratio}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d44b150-27c8-477e-9b89-52c1ccdc4306",
   "metadata": {},
   "source": [
    "#### Performance Weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "28f110dd-9b88-484a-838e-836d454d55ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Drawdown: -18.69911248538685\n",
      "Annualized Mean Returns: 2.5581076725826515\n",
      "Annualized Standard Deviation: 6.982425826205701\n",
      "Sharpe Ratio: 0.3512268409191256\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'merged_data' is your DataFrame with all predictors and factors\n",
    "predictors = ['Inflation', 'InterestRate', 'UNRATE', 'CCI', 'VIX', 'D12', 'E12', 'svar']\n",
    "factors = ['Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA']\n",
    "\n",
    "# Initialize Random Forest models for each factor\n",
    "rf_models = {factor: RandomForestRegressor(random_state=0, n_estimators=100) for factor in factors}\n",
    "\n",
    "# Split data and train models\n",
    "split_data = {}\n",
    "historical_sharpes = {}\n",
    "for factor in factors:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(merged_data[predictors], merged_data[factor], test_size=0.2, shuffle=False)\n",
    "    split_data[factor] = (X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    # Train Random Forest model\n",
    "    rf_model = RandomForestRegressor(random_state=0, n_estimators=100)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    rf_models[factor] = rf_model\n",
    "    \n",
    "    # Calculate historical Sharpe ratio using in-sample (training) data\n",
    "    rf_rate = merged_data['RF'].mean()  # Assuming the risk-free rate is provided in 'merged_data'\n",
    "    excess_returns = y_train - rf_rate\n",
    "    sharpe_ratio = excess_returns.mean() / excess_returns.std() * np.sqrt(12)  # Assuming monthly data\n",
    "    historical_sharpes[factor] = sharpe_ratio if not np.isnan(sharpe_ratio) else 0\n",
    "\n",
    "# Normalize the Sharpe ratios to get weights\n",
    "total_sharpe = sum(historical_sharpes.values())\n",
    "weights = {factor: sharpe / total_sharpe for factor, sharpe in historical_sharpes.items()}\n",
    "\n",
    "# Predict out-of-sample data and generate weighted signals\n",
    "weighted_signals = pd.Series(0, index=X_test.index)\n",
    "for factor, rf_model in rf_models.items():\n",
    "    _, X_test, _, _ = split_data[factor]\n",
    "    predictions = rf_model.predict(X_test)\n",
    "    weighted_signals += predictions * weights[factor]\n",
    "\n",
    "# Generate signals from the weighted predictions\n",
    "composite_signals = np.sign(weighted_signals)\n",
    "\n",
    "# Shift signals to avoid lookahead bias\n",
    "shifted_composite_signals = np.roll(composite_signals, shift=1)\n",
    "shifted_composite_signals[0] = 0  # First signal is unusable\n",
    "\n",
    "# Apply the strategy\n",
    "strategy_returns = pd.Series(0, index=y_test.index)\n",
    "for factor in factors:\n",
    "    _, _, _, y_test = split_data[factor]\n",
    "    strategy_returns += (y_test * shifted_composite_signals) * weights[factor]\n",
    "\n",
    "# Calculate performance metrics\n",
    "cumulative_returns = (1 + strategy_returns).cumprod() - 1\n",
    "rolling_max = cumulative_returns.cummax()\n",
    "drawdowns = (cumulative_returns - rolling_max) / rolling_max\n",
    "max_drawdown = drawdowns.min()\n",
    "annualized_mean_returns = strategy_returns.mean() * 12  # Assuming monthly data\n",
    "annualized_std_dev = strategy_returns.std() * np.sqrt(12)  # Assuming monthly data\n",
    "sharpe_ratio = (annualized_mean_returns - rf_rate) / annualized_std_dev\n",
    "\n",
    "# Output the metrics\n",
    "print(f\"Max Drawdown: {max_drawdown}\")\n",
    "print(f\"Annualized Mean Returns: {annualized_mean_returns}\")\n",
    "print(f\"Annualized Standard Deviation: {annualized_std_dev}\")\n",
    "print(f\"Sharpe Ratio: {sharpe_ratio}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b775a3d7-a2be-4d8e-960a-a6c05ce4717f",
   "metadata": {},
   "source": [
    "#### Confidence Weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "385ffe58-6479-4a2e-a088-38fb30e7da37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Drawdown: -2.934193363684687\n",
      "Annualized Mean Returns: 10.18324007892646\n",
      "Annualized Standard Deviation: 16.783630726727097\n",
      "Sharpe Ratio: 0.6004390787260445\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'merged_data' is your DataFrame with all predictors and factors\n",
    "predictors = ['Inflation', 'InterestRate', 'UNRATE', 'CCI', 'VIX', 'D12', 'E12', 'svar']\n",
    "factors = ['Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA']\n",
    "\n",
    "# Initialize Random Forest models for each factor\n",
    "rf_models = {factor: RandomForestRegressor(random_state=0, n_estimators=100) for factor in factors}\n",
    "\n",
    "# Split data and train models\n",
    "split_data = {}\n",
    "confidence_scores = {}\n",
    "for factor in factors:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(merged_data[predictors], merged_data[factor], test_size=0.2, shuffle=False)\n",
    "    split_data[factor] = (X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    # Train Random Forest model\n",
    "    rf_model = RandomForestRegressor(random_state=0, n_estimators=100)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    rf_models[factor] = rf_model\n",
    "    \n",
    "    # Generate predictions and calculate confidence scores for the out-of-sample data\n",
    "    predictions = rf_model.predict(X_test)\n",
    "    confidence_scores[factor] = np.abs(predictions)  # Using absolute value as a proxy for confidence\n",
    "\n",
    "# Normalize confidence scores to sum to 1\n",
    "total_confidence = np.sum([scores for scores in confidence_scores.values()], axis=0)\n",
    "normalized_confidence = {factor: confidence_scores[factor] / total_confidence for factor in factors}\n",
    "\n",
    "# Create weighted signals based on model confidence\n",
    "weighted_signals = pd.Series(0, index=X_test.index)\n",
    "for factor in factors:\n",
    "    _, X_test, _, _ = split_data[factor]\n",
    "    weighted_signals += np.sign(rf_models[factor].predict(X_test)) * normalized_confidence[factor]\n",
    "\n",
    "# Shift signals to avoid lookahead bias\n",
    "shifted_composite_signals = np.roll(weighted_signals, shift=1)\n",
    "shifted_composite_signals[0] = 0  # First signal is unusable\n",
    "\n",
    "# Apply the strategy\n",
    "strategy_returns = pd.Series(0, index=y_test.index)\n",
    "for factor in factors:\n",
    "    _, _, _, y_test = split_data[factor]\n",
    "    strategy_returns += y_test * shifted_composite_signals\n",
    "\n",
    "# Calculate performance metrics\n",
    "cumulative_returns = (1 + strategy_returns).cumprod() - 1\n",
    "rolling_max = cumulative_returns.cummax()\n",
    "drawdowns = (cumulative_returns - rolling_max) / rolling_max\n",
    "max_drawdown = drawdowns.min()\n",
    "annualized_mean_returns = strategy_returns.mean() * 12  # Assuming monthly data\n",
    "annualized_std_dev = strategy_returns.std() * np.sqrt(12)  # Assuming monthly data\n",
    "sharpe_ratio = (annualized_mean_returns - merged_data['RF'].mean()) / annualized_std_dev\n",
    "\n",
    "# Output the metrics\n",
    "print(f\"Max Drawdown: {max_drawdown}\")\n",
    "print(f\"Annualized Mean Returns: {annualized_mean_returns}\")\n",
    "print(f\"Annualized Standard Deviation: {annualized_std_dev}\")\n",
    "print(f\"Sharpe Ratio: {sharpe_ratio}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daadaa10-2680-422f-ab07-f9a8985db6f1",
   "metadata": {},
   "source": [
    "#### Volatility Weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "d52161ce-78fb-40ad-9894-4e8d9b9e60da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Drawdown: -21731.877789500508\n",
      "Annualized Mean Returns: 20.262480561701466\n",
      "Annualized Standard Deviation: 12.269079826637428\n",
      "Sharpe Ratio: 1.6428932355828925\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'merged_data' is your DataFrame with all predictors and factors\n",
    "predictors = ['Inflation', 'InterestRate', 'UNRATE', 'CCI', 'VIX', 'D12', 'E12', 'svar']\n",
    "factors = ['Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA']\n",
    "\n",
    "# Initialize Random Forest models for each factor\n",
    "rf_models = {factor: RandomForestRegressor(random_state=0, n_estimators=100) for factor in factors}\n",
    "\n",
    "# Train-test split and train models\n",
    "split_data = {}\n",
    "factor_volatility = {}\n",
    "for factor in factors:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(merged_data[predictors], merged_data[factor], test_size=0.2, shuffle=False)\n",
    "    split_data[factor] = (X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    # Train Random Forest model\n",
    "    rf_model = RandomForestRegressor(random_state=0, n_estimators=100)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    rf_models[factor] = rf_model\n",
    "    \n",
    "    # Calculate historical volatility using in-sample (training) data\n",
    "    historical_vol = y_train.rolling(window=12).std().dropna().mean()  # Monthly data, adjust the window as needed\n",
    "    factor_volatility[factor] = historical_vol if not np.isnan(historical_vol) else 0\n",
    "\n",
    "# Invert the volatilities to use them as weights (lower volatility gets higher weight)\n",
    "# Normalize the weights to sum to 1\n",
    "volatility_weights = {factor: 1.0 / vol if vol != 0 else 0 for factor, vol in factor_volatility.items()}\n",
    "total_weight = sum(volatility_weights.values())\n",
    "normalized_weights = {factor: weight / total_weight for factor, weight in volatility_weights.items()}\n",
    "\n",
    "# Generate weighted signals for the out-of-sample data\n",
    "weighted_signals = pd.Series(0, index=X_test.index)\n",
    "for factor in factors:\n",
    "    _, X_test, _, _ = split_data[factor]\n",
    "    predictions = rf_models[factor].predict(X_test)\n",
    "    weighted_signals += np.sign(predictions) * normalized_weights[factor]\n",
    "\n",
    "# Shift signals to avoid lookahead bias\n",
    "shifted_composite_signals = np.roll(weighted_signals, shift=1)\n",
    "shifted_composite_signals[0] = 0  # First signal is unusable\n",
    "\n",
    "# Apply the strategy\n",
    "strategy_returns = pd.Series(0, index=y_test.index)\n",
    "for factor in factors:\n",
    "    _, _, _, y_test = split_data[factor]\n",
    "    strategy_returns += y_test * shifted_composite_signals\n",
    "\n",
    "# Calculate performance metrics\n",
    "cumulative_returns = (1 + strategy_returns).cumprod() - 1\n",
    "rolling_max = cumulative_returns.cummax()\n",
    "drawdowns = (cumulative_returns - rolling_max) / rolling_max\n",
    "max_drawdown = drawdowns.min()\n",
    "annualized_mean_returns = strategy_returns.mean() * 12  # Assuming monthly data\n",
    "annualized_std_dev = strategy_returns.std() * np.sqrt(12)  # Assuming monthly data\n",
    "sharpe_ratio = (annualized_mean_returns - merged_data['RF'].mean()) / annualized_std_dev\n",
    "\n",
    "# Output the metrics\n",
    "print(f\"Max Drawdown: {max_drawdown}\")\n",
    "print(f\"Annualized Mean Returns: {annualized_mean_returns}\")\n",
    "print(f\"Annualized Standard Deviation: {annualized_std_dev}\")\n",
    "print(f\"Sharpe Ratio: {sharpe_ratio}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab11ced-96c2-4c30-ad98-4b8b72241c05",
   "metadata": {},
   "source": [
    "### Volatility Weighting with Stop Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "edcb4a58-1e21-4e82-bdfa-7ae6195e5a9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Drawdown: -0.19008303681453606\n",
      "Annualized Mean Returns: 21.715239088987108\n",
      "Annualized Standard Deviation: 11.952908647779978\n",
      "Sharpe Ratio: 1.8078902314131176\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming 'merged_data' is your DataFrame with all predictors and factors\n",
    "predictors = ['Inflation', 'InterestRate', 'UNRATE', 'CCI', 'VIX', 'D12', 'E12', 'svar']\n",
    "factors = ['Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA']\n",
    "\n",
    "# Initialize Random Forest models for each factor\n",
    "rf_models = {factor: RandomForestRegressor(random_state=0, n_estimators=100) for factor in factors}\n",
    "\n",
    "# Train-test split and train models\n",
    "split_data = {}\n",
    "factor_volatility = {}\n",
    "for factor in factors:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(merged_data[predictors], merged_data[factor], test_size=0.2, shuffle=False)\n",
    "    split_data[factor] = (X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    # Train Random Forest model\n",
    "    rf_model = RandomForestRegressor(random_state=0, n_estimators=100)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    rf_models[factor] = rf_model\n",
    "    \n",
    "    # Calculate historical volatility using in-sample (training) data\n",
    "    historical_vol = y_train.rolling(window=12).std().dropna().mean()  # Monthly data, adjust the window as needed\n",
    "    factor_volatility[factor] = historical_vol if not np.isnan(historical_vol) else 0\n",
    "\n",
    "# Invert the volatilities to use them as weights (lower volatility gets higher weight)\n",
    "# Normalize the weights to sum to 1\n",
    "volatility_weights = {factor: 1.0 / vol if vol != 0 else 0 for factor, vol in factor_volatility.items()}\n",
    "total_weight = sum(volatility_weights.values())\n",
    "normalized_weights = {factor: weight / total_weight for factor, weight in volatility_weights.items()}\n",
    "\n",
    "# Generate weighted signals for the out-of-sample data\n",
    "weighted_signals = pd.Series(0, index=X_test.index)\n",
    "for factor in factors:\n",
    "    _, X_test, _, _ = split_data[factor]\n",
    "    predictions = rf_models[factor].predict(X_test)\n",
    "    weighted_signals += np.sign(predictions) * normalized_weights[factor]\n",
    "\n",
    "# Shift signals to avoid lookahead bias\n",
    "shifted_composite_signals = np.roll(weighted_signals, shift=1)\n",
    "shifted_composite_signals[0] = 0  # First signal is unusable\n",
    "\n",
    "# Apply the strategy\n",
    "strategy_returns = pd.Series(0, index=y_test.index)\n",
    "for factor in factors:\n",
    "    _, _, _, y_test = split_data[factor]\n",
    "    strategy_returns += y_test * shifted_composite_signals\n",
    "\n",
    "# Stop-loss \n",
    "stop_loss_ratio = 0.1\n",
    "for i in range(len(strategy_returns)):\n",
    "    if strategy_returns[i] < -stop_loss_ratio:\n",
    "        strategy_returns[i] = -stop_loss_ratio\n",
    "\n",
    "# Calculate performance metrics\n",
    "cumulative_returns = (1 + strategy_returns).cumprod() - 1\n",
    "rolling_max = cumulative_returns.cummax()\n",
    "drawdowns = (cumulative_returns - rolling_max) / rolling_max\n",
    "max_drawdown = drawdowns.min()\n",
    "annualized_mean_returns = strategy_returns.mean() * 12  # Assuming monthly data\n",
    "annualized_std_dev = strategy_returns.std() * np.sqrt(12)  # Assuming monthly data\n",
    "sharpe_ratio = (annualized_mean_returns - merged_data['RF'].mean()) / annualized_std_dev\n",
    "\n",
    "# Output the metrics\n",
    "print(f\"Max Drawdown: {max_drawdown}\")\n",
    "print(f\"Annualized Mean Returns: {annualized_mean_returns}\")\n",
    "print(f\"Annualized Standard Deviation: {annualized_std_dev}\")\n",
    "print(f\"Sharpe Ratio: {sharpe_ratio}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76be7ed-ed2f-4d55-8fff-03dc6421d7ab",
   "metadata": {},
   "source": [
    "## Testing Quadratic Costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "4e0f3871-3880-4592-995b-6b593b20f586",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_with_quadratic_costs(strategy_signals, initial_capital, alpha, strategy_returns):\n",
    "    \"\"\"\n",
    "    Simulate strategy performance considering quadratic transaction costs.\n",
    "    - strategy_signals: trading signals from the Random Forest model.\n",
    "    - initial_capital: starting capital for the simulation.\n",
    "    - alpha: coefficient for quadratic transaction costs.\n",
    "    - strategy_returns: actual returns based on RF model predictions.\n",
    "    \"\"\"\n",
    "    # Initialize the array for positions and net returns\n",
    "    positions = np.zeros(len(strategy_signals))\n",
    "    positions[0] = initial_capital  # Start with the initial capital\n",
    "\n",
    "    net_returns = np.zeros_like(positions)\n",
    "    net_returns[0] = strategy_returns.iloc[0]\n",
    "\n",
    "    # Calculate positions and apply transaction costs\n",
    "    for i in range(1, len(strategy_signals)):\n",
    "        # Predicted change in position based on the signal\n",
    "        new_position = positions[i-1] * (1 + strategy_returns.iloc[i])\n",
    "\n",
    "        # Volume traded is based on the absolute change\n",
    "        volume_traded = np.abs(new_position - positions[i-1])\n",
    "\n",
    "        # Calculate quadratic transaction costs\n",
    "        transaction_costs = alpha * volume_traded ** 2\n",
    "\n",
    "        # Update positions considering transaction costs\n",
    "        positions[i] = positions[i-1] + (new_position - positions[i-1]) - transaction_costs\n",
    "\n",
    "        # Calculate net returns after transaction costs\n",
    "        net_returns[i] = (positions[i] - positions[i-1]) / positions[i-1] if positions[i-1] != 0 else 0\n",
    "\n",
    "    # Calculate cumulative returns\n",
    "    cumulative_returns = (1 + pd.Series(net_returns)).cumprod() - 1\n",
    "\n",
    "    return cumulative_returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "20357155-f31d-4362-85e2-7fce6627da94",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_signals_and_returns(rf_models, split_data):\n",
    "    \"\"\"Generate trading signals and returns from Random Forest model predictions.\"\"\"\n",
    "    strategy_signals = pd.DataFrame(index=split_data[list(factors)[0]][1].index)  # Assuming all factors have the same test set index\n",
    "    strategy_returns = pd.DataFrame(index=strategy_signals.index)\n",
    "\n",
    "    for factor, model_data in rf_models.items():\n",
    "        X_test = split_data[factor][1]\n",
    "        predictions = model_data.predict(X_test)\n",
    "        \n",
    "        # Generate signals (buy if prediction is positive, sell if negative)\n",
    "        signals = np.sign(predictions)\n",
    "        \n",
    "        # Calculate returns (assuming returns are proportional to the magnitude of predictions for simplicity)\n",
    "        returns = predictions / 100  # Example: Scale down predictions to simulate realistic returns\n",
    "\n",
    "        strategy_signals[factor] = signals\n",
    "        strategy_returns[factor] = returns\n",
    "\n",
    "    # Aggregate signals and returns (average across factors)\n",
    "    strategy_signals['combined'] = strategy_signals.mean(axis=1)\n",
    "    strategy_returns['combined'] = strategy_returns.mean(axis=1)\n",
    "\n",
    "    return strategy_signals['combined'], strategy_returns['combined']\n",
    "\n",
    "# Generate signals and returns\n",
    "strategy_signals, strategy_returns = generate_signals_and_returns(rf_models, split_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "12109d84-ff7e-4e36-b642-3d694e6a6854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Capital: 100000\n",
      "  Alpha 1e-06: Final Cumulative Return: 0.06922872594825491\n",
      "  Alpha 5e-06: Final Cumulative Return: 0.06910086946827687\n",
      "  Alpha 1e-05: Final Cumulative Return: 0.0689410859034143\n",
      "\n",
      "Initial Capital: 500000\n",
      "  Alpha 1e-06: Final Cumulative Return: 0.06910086946827709\n",
      "  Alpha 5e-06: Final Cumulative Return: 0.06846198196956088\n",
      "  Alpha 1e-05: Final Cumulative Return: 0.06766429707492749\n",
      "\n",
      "Initial Capital: 1000000\n",
      "  Alpha 1e-06: Final Cumulative Return: 0.06894108590341386\n",
      "  Alpha 5e-06: Final Cumulative Return: 0.06766429707492749\n",
      "  Alpha 1e-05: Final Cumulative Return: 0.0660720019762322\n",
      "\n",
      "Initial Capital: 5000000\n",
      "  Alpha 1e-06: Final Cumulative Return: 0.06766429707492727\n",
      "  Alpha 5e-06: Final Cumulative Return: 0.061319584078664224\n",
      "  Alpha 1e-05: Final Cumulative Return: 0.05347958716652457\n",
      "\n"
     ]
    }
   ],
   "source": [
    "capitals = [100000, 500000, 1000000, 5000000]\n",
    "alphas = [1e-6, 5e-6, 1e-5]\n",
    "for capital in capitals:\n",
    "    print(f\"Initial Capital: {capital}\")\n",
    "    for alpha in alphas:\n",
    "        cumulative_returns = simulate_with_quadratic_costs(strategy_signals, capital, alpha, strategy_returns)\n",
    "        print(f\"  Alpha {alpha}: Final Cumulative Return: {cumulative_returns.iloc[-1]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b22b8cb-d6d8-449e-a3d3-e7c8b80e5974",
   "metadata": {},
   "source": [
    "## Predictions using lookahead method (PythonPractice8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62dc0b0d-8731-4b66-8ac4-80dedd52c5b3",
   "metadata": {},
   "source": [
    "### Mkt-RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "8415400c-facd-4cb0-8392-cbc5f23c2e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Drawdown: 0.0\n",
      "Annualized Mean Returns: 6.831428571428571\n",
      "Annualized Standard Deviation: 9.629027885616388\n",
      "Sharpe Ratio: 0.6984854902936783\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming 'merged_data' is already loaded with your data\n",
    "predictors = ['Inflation', 'InterestRate', 'UNRATE', 'CCI', 'VIX', 'D12', 'E12', 'svar']\n",
    "factor = 'Mkt-RF'  # Example factor\n",
    "\n",
    "# Preparing the data\n",
    "merged_data['lookahead_returns'] = merged_data[factor].shift(-1)  # Create lookahead returns\n",
    "\n",
    "X = merged_data[predictors].dropna()\n",
    "y = merged_data['lookahead_returns'].dropna()\n",
    "\n",
    "# Ensure alignment of X and y after shifting and dropping NA\n",
    "X = X.loc[y.index]\n",
    "\n",
    "# Split the data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# Train the Random Forest model\n",
    "rf_model = RandomForestRegressor(random_state=0, n_estimators=100)\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict lookahead returns\n",
    "predicted_lookahead_returns = rf_model.predict(X_test)\n",
    "\n",
    "# Define thresholds for confidence in predicted returns\n",
    "upper_threshold = 0.02  # 2% upward movement\n",
    "lower_threshold = -0.02  # 2% downward movement\n",
    "\n",
    "# Generate signals based on dynamic thresholds\n",
    "signals = np.where(predicted_lookahead_returns > upper_threshold, 1,\n",
    "                   np.where(predicted_lookahead_returns < lower_threshold, -1, 0))\n",
    "\n",
    "shifted_signals = np.roll(signals, shift=1)\n",
    "shifted_signals[0] = 0  # Adjust the first signal\n",
    "\n",
    "# Check for indices within the dataset\n",
    "future_index = X_test.index + pd.DateOffset(months=1)\n",
    "future_index = future_index[future_index.isin(merged_data.index)]  # Check if future_index exists in the data\n",
    "\n",
    "if len(future_index) > 0:\n",
    "    actual_future_returns = merged_data.loc[future_index, factor]\n",
    "\n",
    "    # Ensure the length of signals and actual future returns match\n",
    "    if len(shifted_signals) > len(actual_future_returns):\n",
    "        shifted_signals = shifted_signals[:len(actual_future_returns)]\n",
    "\n",
    "    # Calculate strategy returns\n",
    "    strategy_returns = actual_future_returns.reset_index(drop=True) * pd.Series(shifted_signals).reset_index(drop=True)\n",
    "\n",
    "    # Calculate performance metrics\n",
    "    cumulative_returns = (1 + strategy_returns).cumprod() - 1\n",
    "    max_drawdown = (cumulative_returns.cummax() - cumulative_returns).min()\n",
    "    annualized_mean_returns = strategy_returns.mean() * 12\n",
    "    annualized_std_dev = strategy_returns.std() * np.sqrt(12)\n",
    "    average_monthly_rf_rate = merged_data['RF'].mean()\n",
    "    sharpe_ratio = (annualized_mean_returns - average_monthly_rf_rate) / annualized_std_dev\n",
    "\n",
    "    # Output the performance metrics\n",
    "    print(f\"Max Drawdown: {max_drawdown}\")\n",
    "    print(f\"Annualized Mean Returns: {annualized_mean_returns}\")\n",
    "    print(f\"Annualized Standard Deviation: {annualized_std_dev}\")\n",
    "    print(f\"Sharpe Ratio: {sharpe_ratio}\")\n",
    "else:\n",
    "    print(\"No data available for the calculated future index.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e67b487-cfb3-4782-abb9-cdea59c67ad4",
   "metadata": {},
   "source": [
    "### Volatility-based weighting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "ea10b6e2-55d7-4196-b937-7cfda8341a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Drawdown: -27.96621305346532\n",
      "Annualized Mean Returns: 12.859969395406022\n",
      "Annualized Standard Deviation: 9.751329665586944\n",
      "Sharpe Ratio: 1.3079526100655134\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming 'merged_data' is your DataFrame with all predictors and factors\n",
    "predictors = ['Inflation', 'InterestRate', 'UNRATE', 'CCI', 'VIX', 'D12', 'E12', 'svar']\n",
    "factors = ['Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA']\n",
    "\n",
    "# Initialize Random Forest models for each factor\n",
    "rf_models = {}\n",
    "\n",
    "# Initialize data structures\n",
    "split_data = {}\n",
    "factor_volatility = {}\n",
    "\n",
    "# Adding lookahead returns for each factor to the dataset\n",
    "for factor in factors:\n",
    "    merged_data[factor + '_lookahead'] = merged_data[factor].shift(-1)\n",
    "\n",
    "for factor in factors:\n",
    "    # Prepare the dataset for this factor\n",
    "    y = merged_data[factor + '_lookahead'].dropna()  # Target variable\n",
    "    X = merged_data[predictors].loc[y.index]  # Aligning predictors with the target\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "    split_data[factor] = (X_train, X_test, y_train, y_test)\n",
    "\n",
    "    # Train Random Forest model\n",
    "    rf_model = RandomForestRegressor(random_state=0, n_estimators=100)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    rf_models[factor] = rf_model\n",
    "\n",
    "    # Calculate historical volatility using in-sample (training) data\n",
    "    historical_vol = y_train.rolling(window=12).std().dropna().mean()\n",
    "    factor_volatility[factor] = historical_vol if not np.isnan(historical_vol) else 0\n",
    "\n",
    "# Volatility-based weighting\n",
    "volatility_weights = {f: 1.0 / vol if vol != 0 else 0 for f, vol in factor_volatility.items()}\n",
    "total_weight = sum(volatility_weights.values())\n",
    "normalized_weights = {f: weight / total_weight for f, weight in volatility_weights.items()}\n",
    "\n",
    "# Generate weighted signals for the out-of-sample data\n",
    "weighted_signals = pd.Series(0, index=X_test.index)\n",
    "for factor, model in rf_models.items():\n",
    "    _, X_test, _, _ = split_data[factor]\n",
    "    predictions = model.predict(X_test)\n",
    "    weighted_signals += np.sign(predictions) * normalized_weights[factor]\n",
    "\n",
    "# Shift signals to avoid lookahead bias\n",
    "shifted_composite_signals = np.roll(weighted_signals, shift=1)\n",
    "shifted_composite_signals[0] = 0  # First signal is unusable\n",
    "\n",
    "# Apply the strategy\n",
    "strategy_returns = pd.Series(0, index=y_test.index)\n",
    "for factor in factors:\n",
    "    _, _, _, y_test = split_data[factor]\n",
    "    strategy_returns += y_test * shifted_composite_signals\n",
    "\n",
    "# Calculate performance metrics\n",
    "cumulative_returns = (1 + strategy_returns).cumprod() - 1\n",
    "rolling_max = cumulative_returns.cummax()\n",
    "drawdowns = (cumulative_returns - rolling_max) / rolling_max\n",
    "max_drawdown = drawdowns.min()\n",
    "annualized_mean_returns = strategy_returns.mean() * 12\n",
    "annualized_std_dev = strategy_returns.std() * np.sqrt(12)\n",
    "sharpe_ratio = (annualized_mean_returns - merged_data['RF'].mean()) / annualized_std_dev\n",
    "\n",
    "# Output the metrics\n",
    "print(f\"Max Drawdown: {max_drawdown}\")\n",
    "print(f\"Annualized Mean Returns: {annualized_mean_returns}\")\n",
    "print(f\"Annualized Standard Deviation: {annualized_std_dev}\")\n",
    "print(f\"Sharpe Ratio: {sharpe_ratio}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "090ccf74-82c5-4034-b9bb-23e98bc6a1bd",
   "metadata": {},
   "source": [
    "### Performance Weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "3f8189aa-e62d-48ce-ab65-0c3b9f0ecfdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max Drawdown: -7.955776432671923\n",
      "Annualized Mean Returns: 0.977188290073734\n",
      "Annualized Standard Deviation: 15.32608583806793\n",
      "Sharpe Ratio: 0.0568635717944857\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming 'merged_data' is your DataFrame with all predictors and factors\n",
    "predictors = ['Inflation', 'InterestRate', 'UNRATE', 'CCI', 'VIX', 'D12', 'E12', 'svar']\n",
    "factors = ['Mkt-RF', 'SMB', 'HML', 'RMW', 'CMA']\n",
    "\n",
    "# Initialize Random Forest models for each factor\n",
    "rf_models = {}\n",
    "\n",
    "# Train-test split and train models, calculate lookahead returns\n",
    "split_data = {}\n",
    "historical_sharpes = {}\n",
    "for factor in factors:\n",
    "    # Adding lookahead returns for each factor to the dataset\n",
    "    merged_data[factor + '_lookahead'] = merged_data[factor].shift(-1)\n",
    "    \n",
    "    # Prepare the dataset for this factor\n",
    "    y = merged_data[factor + '_lookahead'].dropna()  # Use lookahead returns as the target\n",
    "    X = merged_data[predictors].loc[y.index]  # Align predictors with the target\n",
    "    \n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "    split_data[factor] = (X_train, X_test, y_train, y_test)\n",
    "    \n",
    "    # Train Random Forest model\n",
    "    rf_model = RandomForestRegressor(random_state=0, n_estimators=100)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    rf_models[factor] = rf_model\n",
    "    \n",
    "    # Calculate historical Sharpe ratio using in-sample (training) lookahead data\n",
    "    rf_rate = merged_data['RF'].mean()  # Assuming the risk-free rate is provided in 'merged_data'\n",
    "    excess_returns = (y_train - rf_rate) / y_train.std()\n",
    "    sharpe_ratio = excess_returns.mean() / excess_returns.std() * np.sqrt(12)\n",
    "    historical_sharpes[factor] = sharpe_ratio if not np.isnan(sharpe_ratio) else 0\n",
    "\n",
    "# Normalize the Sharpe ratios to get weights\n",
    "total_sharpe = sum(historical_sharpes.values())\n",
    "weights = {factor: sharpe / total_sharpe for factor, sharpe in historical_sharpes.items()}\n",
    "\n",
    "# Predict out-of-sample data and generate weighted signals\n",
    "weighted_signals = pd.Series(0, index=X_test.index)\n",
    "for factor, model in rf_models.items():\n",
    "    _, X_test, _, _ = split_data[factor]\n",
    "    predictions = model.predict(X_test)\n",
    "    weighted_signals += np.sign(predictions) * weights[factor]\n",
    "\n",
    "# Shift signals to avoid lookahead bias\n",
    "shifted_composite_signals = np.roll(weighted_signals, shift=1)\n",
    "shifted_composite_signals[0] = 0  # First signal is unusable\n",
    "\n",
    "# Apply the strategy\n",
    "strategy_returns = pd.Series(0, index=y_test.index)\n",
    "for factor in factors:\n",
    "    _, _, _, y_test = split_data[factor]\n",
    "    strategy_returns += y_test * shifted_composite_signals\n",
    "\n",
    "# Calculate performance metrics\n",
    "cumulative_returns = (1 + strategy_returns).cumprod() - 1\n",
    "rolling_max = cumulative_returns.cummax()\n",
    "drawdowns = (cumulative_returns - rolling_max) / rolling_max\n",
    "max_drawdown = drawdowns.min()\n",
    "annualized_mean_returns = strategy_returns.mean() * 12\n",
    "annualized_std_dev = strategy_returns.std() * np.sqrt(12)\n",
    "sharpe_ratio = (annualized_mean_returns - rf_rate) / annualized_std_dev\n",
    "\n",
    "# Output the metrics\n",
    "print(f\"Max Drawdown: {max_drawdown}\")\n",
    "print(f\"Annualized Mean Returns: {annualized_mean_returns}\")\n",
    "print(f\"Annualized Standard Deviation: {annualized_std_dev}\")\n",
    "print(f\"Sharpe Ratio: {sharpe_ratio}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda2e4e5-6d1b-41fc-b5b0-54e4ed16683c",
   "metadata": {},
   "source": [
    "### Timing with Optimal Weights (EXERCISE 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "b2aa20d5-a6d9-4a8f-a085-1666a71d7079",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strategy for factor Mkt-RF:\n",
      "Max Drawdown: 0.0\n",
      "Annualized Return: 4.28416837685976\n",
      "Sharpe Ratio: 0.6343104102527062\n",
      "Strategy for factor HML:\n",
      "Max Drawdown: 0.0\n",
      "Annualized Return: -0.03575047115545191\n",
      "Sharpe Ratio: -1.8227987916049775\n",
      "Strategy for factor SMB:\n",
      "Max Drawdown: 0.0\n",
      "Annualized Return: 1.8350572057293522\n",
      "Sharpe Ratio: 0.1492554045550133\n",
      "Strategy for factor RMW:\n",
      "Max Drawdown: 0.0\n",
      "Annualized Return: 0.3439392991921937\n",
      "Sharpe Ratio: 0.6350338699597978\n",
      "Strategy for factor CMA:\n",
      "Max Drawdown: 0.0\n",
      "Annualized Return: -5.326521409330862\n",
      "Sharpe Ratio: -1.4534181587096007\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming merged_data is your DataFrame with factors and predictors\n",
    "\n",
    "# Define the factors and predictors\n",
    "factors = ['Mkt-RF', 'HML', 'SMB', 'RMW', 'CMA']\n",
    "predictors = ['Inflation', 'InterestRate', 'UNRATE', 'CCI', 'VIX', 'D12', 'E12', 'svar']\n",
    "gamma = 3  # Risk aversion parameter, modify as needed\n",
    "\n",
    "# Compute excess returns for each factor\n",
    "for factor in factors:\n",
    "    merged_data[factor + '_excess'] = merged_data[factor] - merged_data['RF']\n",
    "\n",
    "# Define a function to perform Random Forest regression and backtesting\n",
    "def random_forest_timing(data, factor, predictors, gamma):\n",
    "    # Prepare the data, align features and target\n",
    "    X = data[predictors].dropna()\n",
    "    y = data[factor + '_excess'].dropna()\n",
    "    X = X.loc[y.index]\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "    # Train the Random Forest model\n",
    "    rf_model = RandomForestRegressor(random_state=0, n_estimators=100)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict lookahead returns\n",
    "    predicted_lookahead_returns = rf_model.predict(X_test)\n",
    "\n",
    "    # Compute optimal weights (w*) using the mean-variance utility function\n",
    "    mu_pred = predicted_lookahead_returns.mean()\n",
    "    var_pred = predicted_lookahead_returns.var()\n",
    "    w_star = mu_pred / (gamma * var_pred)\n",
    "\n",
    "    # Align the signals with the actual future returns\n",
    "    future_returns = y_test.shift(-1).dropna()  # Shift the actual returns for lookahead\n",
    "    signals = pd.Series(np.where(predicted_lookahead_returns[:-1] > 0, 1, -1), index=future_returns.index)\n",
    "\n",
    "    # Calculate weighted strategy returns based on the signals and future returns\n",
    "    strategy_returns = w_star * signals * future_returns\n",
    "\n",
    "    # Calculate performance metrics\n",
    "    cumulative_returns = (1 + strategy_returns).cumprod()\n",
    "    max_drawdown = (cumulative_returns.cummax() - cumulative_returns).min()\n",
    "    annualized_return = strategy_returns.mean() * 12\n",
    "    annualized_std = strategy_returns.std() * np.sqrt(12)\n",
    "    sharpe_ratio = (annualized_return - merged_data['RF'].mean()) / annualized_std\n",
    "\n",
    "    # Output the performance metrics\n",
    "    print(f\"Strategy for factor {factor}:\")\n",
    "    print(f\"Max Drawdown: {max_drawdown}\")\n",
    "    print(f\"Annualized Return: {annualized_return}\")\n",
    "    print(f\"Sharpe Ratio: {sharpe_ratio}\")\n",
    "\n",
    "# Perform the Random Forest timing for each factor\n",
    "for factor in factors:\n",
    "    random_forest_timing(merged_data, factor, predictors, gamma)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
